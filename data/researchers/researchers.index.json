{
  "generated_at": "2026-02-21T10:07:59.186Z",
  "pipeline_version": "v0.1.0",
  "run_config": {
    "model": "qwen3.5-plus",
    "skip_ai": false,
    "full_refresh": false,
    "max_papers": null,
    "delay_ms": 300,
    "concurrency": 32,
    "save_every": 5,
    "researcher_names": [
      "Bin Hu",
      "Bing Qin",
      "Björn Schuller",
      "C.L. Philip Chen",
      "Dan Guo",
      "Dong Zhang",
      "Guanqun Bi",
      "Guiguang Ding",
      "Guoying Zhao",
      "Honghai Liu",
      "Hongxun Yao",
      "Jia Liu",
      "Jianhua Tao",
      "Jufeng Yang",
      "Kejun Zhang",
      "Leida Li",
      "Meng Wang",
      "Ming Li",
      "Peipei Song",
      "Qin Jin",
      "Qunxi Dong",
      "Shengping Zhang",
      "Shiguang Shan",
      "Sicheng Zhao",
      "Su-Jing Wang",
      "Tong Zhang",
      "Weidong Chen",
      "Wenming Zheng",
      "Xiao Sun",
      "Xiaobai Li",
      "Xiaolan Fu",
      "Xun Chen",
      "Yanyan Zhao",
      "Yue Gao",
      "Zheng Lian"
    ]
  },
  "researchers": [
    {
      "identity": {
        "name": "Bin Hu",
        "orcid": "https://orcid.org/0000-0003-3514-5413",
        "google_scholar": "https://scholar.google.com/citations?user=5kcgY8MAAAAJ",
        "openalex_author_id": "A5100380066",
        "openalex_author_url": "https://openalex.org/A5100380066"
      },
      "affiliation": {
        "last_known_institution": "北京理工大学",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 1188,
        "cited_by_count": 23708,
        "h_index": 76,
        "i10_index": 417
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective brain-computer interfaces",
            "weight": 1
          },
          {
            "name": "Clinical emotion AI",
            "weight": 0.98
          },
          {
            "name": "Multimodal emotion sensing",
            "weight": 0.96
          },
          {
            "name": "Computational psychiatry",
            "weight": 0.94
          },
          {
            "name": "AI-driven mental health support",
            "weight": 0.92
          },
          {
            "name": "Interpretable affective AI",
            "weight": 0.89
          },
          {
            "name": "Emotion-aware wearable systems",
            "weight": 0.87
          },
          {
            "name": "Federated and privacy-preserving affective computing",
            "weight": 0.85
          }
        ],
        "trend_summary": "Bin Hu's research centers on bridging neuroscience, clinical psychiatry, and AI to build robust, interpretable, and deployable affective computing systems—especially for depression and anxiety detection and intervention. A dominant trend is the integration of multimodal physiological signals (EEG, fNIRS, eye-tracking, speech, facial, gait) with advanced ML architectures (graph neural networks, transformers, diffusion models, hybrid CNN-LSTM-GCN), grounded in neuroscientific principles (e.g., brain network dynamics, entropy, microstates). There is strong emphasis on real-world applicability: edge/embedded deployment (e.g., three-lead EEG, IoT, VR), cross-subject generalization, privacy preservation (federated learning), and clinical translation—evidenced by frequent use of open clinical datasets, validation against diagnostic scales, and co-design with clinical workflows. Recent work increasingly incorporates LLMs for empathic dialogue, semantic integration, and closed-loop interventions, signaling a shift toward symbiotic human-AI mental health ecosystems.",
        "representative_papers": [
          {
            "title": "FDDGNet: An information bottleneck-inspired feature disentanglement network for cross-subject EEG-based emotion recognition",
            "why": "Exemplifies core focus on cross-subject generalization in affective BCIs using theoretically grounded (information bottleneck) deep learning."
          },
          {
            "title": "EmoSavior: Depression recognition and intervention via multimodal physiological signals and large language models",
            "why": "Represents the integrative frontier—combining multimodal biosensing with LLMs for end-to-end recognition *and* empathic intervention."
          },
          {
            "title": "MAMILS: A Memory-Aware Multiobjective Scheduler for Real-Time Embedded EEG Depression Diagnosis",
            "why": "Highlights commitment to real-world deployment—optimizing embedded, real-time, resource-constrained affective BCI systems."
          },
          {
            "title": "Spatio-temporal fusion of fNIRS signals with multi-view structured sparse canonical correlation analysis for depression detection",
            "why": "Demonstrates methodological rigor in multimodal neuroimaging fusion, grounded in statistical learning and clinical affective assessment."
          },
          {
            "title": "FedVCPL-Diff: A federated convolutional prototype learning framework with a diffusion model for speech emotion recognition",
            "why": "Captures emerging emphasis on privacy (federated learning), robust representation (diffusion models), and paralinguistic affective computing."
          },
          {
            "title": "Explainable Depression Classification Based on EEG Feature Selection From Audio Stimuli",
            "why": "Embodies dual focus on clinical interpretability (explainable AI) and ecologically valid emotion elicitation (audio stimuli)."
          },
          {
            "title": "Machine Learning Enabled Reusable Adhesion, Entangled Network-Based Hydrogel for Long-Term, High-Fidelity EEG Recording and Attention Assessment",
            "why": "Reflects interdisciplinary hardware-software co-design for affective sensing—bridging materials science and affective neuroscience."
          },
          {
            "title": "Toward the Construction of Affective Brain-Computer Interface: A Systematic Review",
            "why": "Synthesizes foundational knowledge and identifies practical challenges (individual differences, real-world deployment), framing the field’s evolution."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 1107,
        "interesting_works_count": 394,
        "new_works_count": 0,
        "deduped_works_count": 1107,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5100380066.json"
    },
    {
      "identity": {
        "name": "Bing Qin",
        "orcid": "https://orcid.org/0000-0002-2543-5604",
        "google_scholar": "https://scholar.google.com/citations?user=LKnCub0AAAAJ",
        "openalex_author_id": "A5017671620",
        "openalex_author_url": "https://openalex.org/A5017671620"
      },
      "affiliation": {
        "last_known_institution": "Sinopec (China)",
        "last_known_country": "China",
        "source": "openalex"
      },
      "metrics": {
        "works_count": 460,
        "cited_by_count": 16213,
        "h_index": 45,
        "i10_index": 153
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective multimodal learning",
            "weight": 1
          },
          {
            "name": "Affective natural language processing",
            "weight": 0.98
          },
          {
            "name": "Affective computing for mental health",
            "weight": 0.96
          },
          {
            "name": "Emotion-aware dialogue systems",
            "weight": 0.93
          },
          {
            "name": "Clinical-scale guided NLP",
            "weight": 0.91
          },
          {
            "name": "Emotion representation learning",
            "weight": 0.89
          },
          {
            "name": "Explainable affective AI",
            "weight": 0.87
          },
          {
            "name": "Affective neuroscience × NLP",
            "weight": 0.84
          }
        ],
        "trend_summary": "Bing Qin's research centers on bridging affective computing with real-world clinical and human-centered applications—especially in mental health screening, empathetic dialogue, and neuro-linguistic modeling. A strong longitudinal emphasis exists on multimodal (text, audio, image, physiological) emotion understanding, increasingly grounded in clinical validity (e.g., psychiatric scales), explainability, and robustness (e.g., speaker generalization, uncertainty awareness, bias mitigation). Recent work shows a pronounced shift toward integrating large language models with emotion science—via prompt engineering, knowledge injection, causal reasoning, and brain–language alignment—while maintaining fine-grained, aspect-level, and distributional modeling of affect. The trajectory reflects a maturing field: from lexical and sentence-level sentiment analysis (2010s) to clinically interpretable, multimodal, neuro-informed, and generative affective AI (2023–2025).",
        "representative_papers": [
          {
            "title": "Face-Sensitive Image-to-Emotional-Text Cross-modal Translation for Multimodal Aspect-based Sentiment Analysis",
            "why": "Highly cited (56) foundational work on cross-modal affective alignment; exemplifies core multimodal + fine-grained + generation focus."
          },
          {
            "title": "Cognitive distortion based explainable depression detection and analysis technologies for the adolescent internet users on social media",
            "why": "Integrates clinical psychology (cognitive distortions) with explainable AI and social media emotion analysis—key intersection of mental health and interpretability."
          },
          {
            "title": "Scale-CoT: Integrating LLM with Psychiatric Scales for Analyzing Mental Health Issues on Social Media",
            "why": "Pioneering framework for clinical-scale-guided LLM reasoning—represents the critical convergence of psychiatry, NLP, and foundation models."
          },
          {
            "title": "Semantic-aware Contrastive Learning for Electroencephalography-to-Text Generation with Curriculum Learning",
            "why": "Embodies the affective neuroscience × NLP direction—directly bridges neural signals (EEG) and emotion-laden language generation."
          },
          {
            "title": "TransESC: Smoothing Emotional Support Conversation via Turn-Level State Transition",
            "why": "Seminal work on emotion-aware, therapeutic dialogue systems—demonstrates deep integration of computational psychology and conversational AI."
          },
          {
            "title": "SDRS: Sentiment-Aware Disentangled Representation Shifting for Multimodal Sentiment Analysis",
            "why": "Exemplifies advanced emotion representation learning—disentanglement, cross-modal alignment, and affective feature control."
          },
          {
            "title": "Generalizing to Unseen Speakers: Multimodal Emotion Recognition in Conversations With Speaker Generalization",
            "why": "Highlights robustness and generalizability—core challenge in real-world affective AI, especially for clinical deployment."
          },
          {
            "title": "Large language models meet text-centric multimodal sentiment analysis: a survey",
            "why": "Authoritative 2025 survey synthesizing the field’s evolution—underscores Qin’s leadership in mapping and unifying affective AI paradigms."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 382,
        "interesting_works_count": 83,
        "new_works_count": 0,
        "deduped_works_count": 382,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5017671620.json"
    },
    {
      "identity": {
        "name": "Björn Schuller",
        "orcid": "https://orcid.org/0000-0002-6478-8699",
        "google_scholar": "https://scholar.google.com/citations?user=TxKNCSoAAAAJ",
        "openalex_author_id": "A5043060302",
        "openalex_author_url": "https://openalex.org/A5043060302"
      },
      "affiliation": {
        "last_known_institution": "Imperial College London",
        "last_known_country": "United Kingdom",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 1764,
        "cited_by_count": 54089,
        "h_index": 102,
        "i10_index": 722
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Speech Emotion Recognition",
            "weight": 1
          },
          {
            "name": "Multimodal Affective Computing",
            "weight": 0.95
          },
          {
            "name": "Computational Paralinguistics",
            "weight": 0.9
          },
          {
            "name": "Mental Health Detection",
            "weight": 0.85
          },
          {
            "name": "Deep Learning Architectures",
            "weight": 0.8
          },
          {
            "name": "Domain Adaptation",
            "weight": 0.7
          },
          {
            "name": "Physiological Signal Processing",
            "weight": 0.6
          },
          {
            "name": "Responsible Affective Computing",
            "weight": 0.5
          }
        ],
        "trend_summary": "Research evolved from foundational acoustic feature engineering (2002-2006) to structured paralinguistic challenges (2007-2011). Early work focused on HMMs and speaker-independent speech emotion recognition. Between 2012-2016, deep learning integration began, emphasizing cross-corpus robustness and standardized feature sets like GeMAPS. The 2017-2021 window saw dominance of end-to-end multimodal architectures and expanded applications into mental health detection, particularly depression and autism. Recent years (2022-2026) prioritize foundation models, large language models, and responsible AI. Current focus shifts towards clinical deployment, privacy-preserving federated learning, and cognition-oriented affective computing. Continuity exists in multimodal fusion, but methodology transitioned from handcrafted features to generative foundation models. Evidence for 2002-2006 is limited compared to later periods, yet establishes core paralinguistic taxonomies. Overall, the trajectory moves from laboratory-constrained acoustic analysis to in-the-wild, ethically-aware mental health interventions powered by general AI.",
        "representative_papers": [
          {
            "title": "The INTERSPEECH 2009 emotion challenge",
            "why": "Established benchmarking standards for speech emotion recognition, catalyzing community progress."
          },
          {
            "title": "Cross-Corpus Acoustic Emotion Recognition: Variances and Strategies",
            "why": "Addresses critical robustness and generalization challenges across different datasets."
          },
          {
            "title": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing",
            "why": "Defines the broader field scope beyond emotion to include personality and paralinguistic traits."
          },
          {
            "title": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing",
            "why": "Standardized acoustic feature extraction, enabling reproducibility across affective studies."
          },
          {
            "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
            "why": "Marks the methodological shift from handcrafted features to end-to-end deep learning."
          },
          {
            "title": "End-to-End Multimodal Emotion Recognition Using Deep Neural Networks",
            "why": "Demonstrates the integration of multiple modalities using advanced neural architectures."
          },
          {
            "title": "Will Affective Computing Emerge From Foundation Models and General Artificial Intelligence? A First Evaluation of ChatGPT",
            "why": "Investigates the impact of large language models and foundation models on emotion tasks."
          },
          {
            "title": "Digital interventions in mental health: An overview and future perspectives",
            "why": "Represents the mature application of affective computing in clinical mental health interventions."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 1568,
        "interesting_works_count": 907,
        "new_works_count": 175,
        "deduped_works_count": 1568,
        "ai_called_count": 175,
        "summary_recomputed": true
      },
      "profile_path": "data/researchers/profiles/A5043060302.json"
    },
    {
      "identity": {
        "name": "C.L. Philip Chen",
        "orcid": "https://orcid.org/0000-0001-5451-7230",
        "google_scholar": "https://scholar.google.com/citations?user=Q5248zwAAAAJ",
        "openalex_author_id": "A5100643265",
        "openalex_author_url": "https://openalex.org/A5100643265"
      },
      "affiliation": {
        "last_known_institution": "University of Macau",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 1448,
        "cited_by_count": 62117,
        "h_index": 126,
        "i10_index": 760
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective brain-computer interfaces",
            "weight": 1
          },
          {
            "name": "Multimodal emotion modeling",
            "weight": 0.97
          },
          {
            "name": "Interpretable AI for emotion decoding",
            "weight": 0.94
          },
          {
            "name": "Graph neural networks for affective states",
            "weight": 0.91
          },
          {
            "name": "Cross-subject and domain-adaptive affective modeling",
            "weight": 0.88
          },
          {
            "name": "Affective neuroscience",
            "weight": 0.85
          },
          {
            "name": "Emotion-aware AI systems",
            "weight": 0.82
          },
          {
            "name": "Physiological signal-based emotion modeling",
            "weight": 0.79
          }
        ],
        "trend_summary": "C.L. Philip Chen's recent research centers on neurophysiologically grounded, interpretable, and robust affective computing—especially EEG-based emotion recognition enhanced by graph neural networks, broad learning, and multimodal fusion. A strong trend is the integration of neuroscientific validity (e.g., functional connectivity, neural correlates) with trustworthy AI techniques (interpretability, uncertainty handling, domain adaptation). Cross-subject generalization, cross-cultural validation, and real-world applicability (wearables, EdTech, driving, robotics) are increasingly emphasized. There is also a growing thread linking affective modeling to clinical applications (e.g., depression detection) and human-centered domains (marketing, social sensing, sarcasm/irony inference), often augmented by LLMs or adaptive architectures.",
        "representative_papers": [
          {
            "title": "Grop: Graph Orthogonal Purification Network for EEG Emotion Recognition",
            "why": "Highest-cited paper in the list (26 citations); exemplifies core themes: graph-based modeling, robustness to individual variability, neuroadaptive systems, and multimodal representation learning."
          },
          {
            "title": "GDDN: Graph Domain Disentanglement Network for Generalizable EEG Emotion Recognition",
            "why": "Highly cited (53) and defines the intersection of graph learning, domain disentanglement, and generalizable affective modeling—central to Chen’s cross-subject focus."
          },
          {
            "title": "Gusa: Graph-Based Unsupervised Subdomain Adaptation for Cross-Subject EEG Emotion Recognition",
            "why": "Top-cited (54) work on unsupervised adaptation; represents the shift toward practical, calibration-free BCI-affective systems."
          },
          {
            "title": "Cross-Cultural Emotion Recognition With EEG and Eye Movement Signals Based on Multiple Stacked Broad Learning System",
            "why": "Most cited paper overall (61); anchors cross-cultural, multimodal, and portable affective sensing—highlighting real-world deployment and diversity-aware design."
          },
          {
            "title": "Fine-Grained Interpretability for EEG Emotion Recognition: Concat-Aided Grad-CAM and Systematic Brain Functional Network",
            "why": "Seminal interpretability work (49 citations); bridges explainable AI with brain functional networks—key for clinical and ethical adoption."
          },
          {
            "title": "Hierarchical Dynamic Graph Convolutional Network With Interpretability for EEG-Based Emotion Recognition",
            "why": "Foundational high-impact paper (86 citations); established dynamic graph learning + interpretability as a dominant paradigm in his group’s EEG emotion work."
          },
          {
            "title": "AdamGraph: Adaptive Attention-Modulated Graph Network for EEG Emotion Recognition",
            "why": "Highly cited (10) and conceptually rich—integrates personalization, interpretability, cross-subject transfer, and multimodal analysis in one framework."
          },
          {
            "title": "Memo2496: Expert-Annotated Dataset and Dual-View Adaptive Framework for Music Emotion Recognition",
            "why": "Represents expansion beyond physiology into ecological, multimodal affect domains (music) with emphasis on dataset curation and feature drift mitigation—critical for real-world affective AI."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 1422,
        "interesting_works_count": 49,
        "new_works_count": 0,
        "deduped_works_count": 1422,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5100643265.json"
    },
    {
      "identity": {
        "name": "Dan Guo",
        "orcid": "https://orcid.org/0000-0003-2594-254X",
        "google_scholar": null,
        "openalex_author_id": "A5059530979",
        "openalex_author_url": "https://openalex.org/A5059530979"
      },
      "affiliation": {
        "last_known_institution": "Institute of Artificial Intelligence, Hefei Comprehensive National Science Center",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 214,
        "cited_by_count": 2957,
        "h_index": 29,
        "i10_index": 65
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "affective computing",
            "weight": 1
          },
          {
            "name": "emotion recognition from subtle behavioral cues",
            "weight": 0.92
          },
          {
            "name": "robust affective signal processing",
            "weight": 0.88
          },
          {
            "name": "affective multimodal fusion",
            "weight": 0.85
          },
          {
            "name": "emotion-aware AI",
            "weight": 0.83
          },
          {
            "name": "computational aesthetics and emotional response modeling",
            "weight": 0.76
          },
          {
            "name": "affective multimedia understanding",
            "weight": 0.72
          },
          {
            "name": "computational psychiatry and clinical emotion assessment",
            "weight": 0.69
          }
        ],
        "trend_summary": "Dan Guo's research centers on robust, interpretable, and clinically grounded affective computing—emphasizing fine-grained, multimodal analysis of subtle emotional signals (e.g., micro-actions, facial dynamics, video-audio-text) under real-world challenges like label noise, distribution shift, and heterogeneity. A strong thread links technical innovation (contrastive learning, distributionally robust optimization, vision-language emotion modeling) with human-centered applications in mental health monitoring, art cognition, education, and healthcare training. Recent work increasingly bridges computational methods with psychological theory and clinical validity.",
        "representative_papers": [
          {
            "title": "Benchmarking Micro-Action Recognition: Dataset, Methods, and Applications",
            "why": "Highly cited foundational work defining the micro-action paradigm for fine-grained affective behavior analysis; establishes benchmarks and unifies evaluation across behavioral signal processing."
          },
          {
            "title": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation",
            "why": "Pioneering integration of large language models with clinical psychology, bridging NLP and computational psychiatry—high citation count reflects impact on emotion-aware AI foundations."
          },
          {
            "title": "Every Subtlety Counts: Fine-grained Person Independence Micro-Action Recognition via Distributionally Robust Optimization",
            "why": "Exemplifies core methodological focus: using DRO to generalize emotion recognition across individuals—a key challenge in real-world affective AI deployment."
          },
          {
            "title": "Emotional Video Captioning With Vision-Based Emotion Interpretation Network",
            "why": "Seminal work in affective multimodal understanding, combining video perception and emotion-conditioned NLG—high citations confirm influence on affective multimedia modeling."
          },
          {
            "title": "Multimodal Depression Estimation via Contrastive Modality Alignment and Fusion",
            "why": "Represents clinical translation focus: rigorous multimodal fusion for depression estimation, aligning with growing emphasis on validated mental health technology."
          },
          {
            "title": "EmoSEM: Segment and Explain Emotion Stimuli in Visual Art",
            "why": "Highlights unique interdisciplinary angle—merging computational aesthetics, affective computing, and interpretability in art contexts, distinguishing Guo’s portfolio."
          },
          {
            "title": "Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition",
            "why": "Key contribution to interpretability and uncertainty handling in fine-grained affective action recognition—high citations signal methodological influence."
          },
          {
            "title": "Alleviating Confirmation Bias in Learning with Noisy Labels via Two-Network Collaboration",
            "why": "Addresses critical real-world constraint—noisy affective annotations—using novel collaborative learning, reflecting commitment to robustness in practical settings."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 200,
        "interesting_works_count": 23,
        "new_works_count": 0,
        "deduped_works_count": 200,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5059530979.json"
    },
    {
      "identity": {
        "name": "Dong Zhang",
        "orcid": "https://orcid.org/0000-0001-6756-6664",
        "google_scholar": "https://scholar.google.com/citations?user=1E_WmCUAAAAJ",
        "openalex_author_id": "A5100366067",
        "openalex_author_url": "https://openalex.org/A5100366067"
      },
      "affiliation": {
        "last_known_institution": "Sun Yat-sen University",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 108,
        "cited_by_count": 1431,
        "h_index": 19,
        "i10_index": 28
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "multimodal emotion analysis",
            "weight": 1
          },
          {
            "name": "affective computing",
            "weight": 0.95
          },
          {
            "name": "emotion-mediated consumer decision-making",
            "weight": 0.88
          },
          {
            "name": "fine-grained emotion/sentiment modeling",
            "weight": 0.82
          },
          {
            "name": "cross-modal affective signal integration",
            "weight": 0.76
          },
          {
            "name": "affective speech synthesis",
            "weight": 0.71
          },
          {
            "name": "emotion and economic decision-making",
            "weight": 0.65
          },
          {
            "name": "fuzzy affective modeling",
            "weight": 0.59
          }
        ],
        "trend_summary": "Dong Zhang's research centers on the intersection of affective science and AI-driven decision systems, with a strong longitudinal emphasis on multimodal emotion analysis—spanning text, speech, and physiological signals—and its application in real-world domains including marketing, rural development, and human-centered AI. Early work established foundations in fuzzy and computational sentiment modeling for decision support; mid-career contributions advanced graph- and attention-based multimodal emotion recognition; recent work expands into affective speech alignment, neuromarketing, and socio-economic implications of emotional mechanisms in poverty and agriculture—reflecting a convergent trend toward context-aware, human-aligned affective intelligence.",
        "representative_papers": [
          {
            "title": "Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection",
            "why": "Highly cited (122) foundational paper integrating cross-modal affective signals with fine-grained aspect-level sentiment, exemplifying core methodology and impact."
          },
          {
            "title": "Modeling both Context- and Speaker-Sensitive Dependence for Emotion Detection in Multi-speaker Conversations",
            "why": "Most cited paper (210) introducing speaker-aware, graph-based emotion modeling in dialogue—seminal for conversational affective computing."
          },
          {
            "title": "Psychological Poverty Traps in Rural Farm Households: Implications for Sustainable Agricultural Development and Rural Revitalization in China",
            "why": "Represents newest interdisciplinary expansion—bridging affective mechanisms with behavioral economics and rural policy, signaling strategic domain diversification."
          },
          {
            "title": "SpeechAlign: Aligning Speech Generation to Human Preferences",
            "why": "Flagship 2024 work on affective speech synthesis and preference-based prosody modeling, highlighting shift toward human-centered expressive AI evaluation."
          },
          {
            "title": "A Review of Data-Driven Techniques for Neuromarketing",
            "why": "Synthesizes affective computing with neural/physiological data in marketing—a key integrative survey framing emerging applications in consumer neuroscience."
          },
          {
            "title": "The role of key online reviews in affecting online hotel booking: an empirical investigation",
            "why": "Early high-impact (32 citations) empirical study linking affective review content to behavioral outcomes in hospitality, anchoring applied affective decision research."
          },
          {
            "title": "Multi-modal Multi-label Emotion Recognition with Heterogeneous Hierarchical Message Passing",
            "why": "Pioneering graph-based dependency modeling for partial time-series emotion recognition—technically influential in multimodal affective architectures."
          },
          {
            "title": "An extended TODIM method to rank products with online reviews under intuitionistic fuzzy environment",
            "why": "Foundational contribution to fuzzy affective reasoning in recommender systems, establishing early methodological rigor in emotion-aware decision modeling."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 103,
        "interesting_works_count": 12,
        "new_works_count": 0,
        "deduped_works_count": 103,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5100366067.json"
    },
    {
      "identity": {
        "name": "Guanqun Bi",
        "orcid": "https://orcid.org/0000-0001-8829-9489",
        "google_scholar": "https://scholar.google.com/citations?user=cTMKxukAAAAJ",
        "openalex_author_id": "A5016119118",
        "openalex_author_url": "https://openalex.org/A5016119118"
      },
      "affiliation": {
        "last_known_institution": null,
        "last_known_country": null,
        "source": "openalex"
      },
      "metrics": {
        "works_count": 22,
        "cited_by_count": 112,
        "h_index": 5,
        "i10_index": 1
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Emotion-aware dialogue systems",
            "weight": 1
          },
          {
            "name": "Affective computing in mental health AI",
            "weight": 0.95
          },
          {
            "name": "Computational modeling of empathy",
            "weight": 0.92
          },
          {
            "name": "Affective natural language processing",
            "weight": 0.88
          },
          {
            "name": "Human-AI alignment in emotional support contexts",
            "weight": 0.85
          },
          {
            "name": "Cognitive-affective modeling in NLP",
            "weight": 0.82
          },
          {
            "name": "Psychology-guided AI frameworks",
            "weight": 0.78
          },
          {
            "name": "Multi-perspective affective assessment",
            "weight": 0.75
          }
        ],
        "trend_summary": "Guanqun Bi's research centers on bridging affective science and AI—particularly LLMs—to advance emotionally intelligent, clinically grounded, and human-aligned systems for mental health support. A strong longitudinal trend emerges: evolving from foundational affective NLP (e.g., empathetic response generation) toward integrated, psychology-informed architectures (e.g., theory-of-mind benchmarking, tripartite feedback loops, social simulation), with increasing emphasis on rigorous evaluation, controllability, clinical validity, and neurodiverse inclusivity. The work consistently prioritizes interpretability, human-centered design, and closed-loop optimization in real-world therapeutic contexts.",
        "representative_papers": [
          {
            "title": "DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation",
            "why": "Seminal methodological contribution introducing controllable, fine-grained affective generation using diffusion models—highly cited (7) and foundational for emotion-aware conversational AI."
          },
          {
            "title": "ToMBench: Benchmarking Theory of Mind in Large Language Models",
            "why": "Pioneering benchmark that bridges cognitive psychology and LLM evaluation, enabling systematic assessment of social reasoning—a key pillar of empathic AI."
          },
          {
            "title": "Seri: Sketching-Reasoning-Integrating Progressive Workflow for Empathetic Response Generation",
            "why": "Introduces a human-inspired, modular cognitive-affective workflow for empathetic generation—early influential framework for affective reasoning in language models."
          },
          {
            "title": "Ψ-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback",
            "why": "Represents the maturation of Bi's work into interactive, multi-stakeholder evaluation and closed-loop optimization—core to current clinical AI alignment efforts."
          },
          {
            "title": "SocialSim: Towards Socialized Simulation of Emotional Support Conversation",
            "why": "Advances computational psychology by formalizing social-emotional dynamics in simulation—key for safe, scalable training and testing of mental health AI."
          },
          {
            "title": "SS-GEN: A Social Story Generation Framework with Large Language Models",
            "why": "Exemplifies applied, neurodiversity-focused affective AI—translating psychological principles (social stories) into LLM-driven interventions."
          },
          {
            "title": "MAGI: Multi-Agent Guided Interview for Psychiatric Assessment",
            "why": "Demonstrates clinical translation—integrating multi-agent reasoning with psychiatric domain knowledge for structured, emotion-aware screening."
          },
          {
            "title": "PsychePass: Calibrating LLM Therapeutic Competence via Trajectory-Anchored Tournaments",
            "why": "Latest conceptual leap: reframing therapeutic competence as longitudinal, trajectory-based skill calibration—setting new standards for human-AI alignment in psychotherapy AI."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 16,
        "interesting_works_count": 11,
        "new_works_count": 0,
        "deduped_works_count": 16,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5016119118.json"
    },
    {
      "identity": {
        "name": "Guiguang Ding",
        "orcid": "https://orcid.org/0000-0003-0137-9975",
        "google_scholar": "https://scholar.google.com/citations?user=6cYECZAAAAAJ",
        "openalex_author_id": "A5057732142",
        "openalex_author_url": "https://openalex.org/A5057732142"
      },
      "affiliation": {
        "last_known_institution": "Harbin University",
        "last_known_country": "China",
        "source": "openalex"
      },
      "metrics": {
        "works_count": 321,
        "cited_by_count": 20109,
        "h_index": 62,
        "i10_index": 147
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "affective computing",
            "weight": 0.102
          },
          {
            "name": "personalized emotion prediction",
            "weight": 0.02
          },
          {
            "name": "affective computer vision",
            "weight": 0.02
          },
          {
            "name": "cross-domain emotion modeling",
            "weight": 0.02
          },
          {
            "name": "personalized emotion modeling",
            "weight": 0.02
          },
          {
            "name": "affective image analysis",
            "weight": 0.02
          },
          {
            "name": "subjective emotion modeling",
            "weight": 0.02
          },
          {
            "name": "affective multimodal learning",
            "weight": 0.01
          }
        ],
        "trend_summary": "AI summary unavailable. Generated by frequency fallback.",
        "representative_papers": [
          {
            "title": "Continuous Probability Distribution Prediction of Image Emotions via Multitask Shared Sparse Regression",
            "why": "Highly cited (191)."
          },
          {
            "title": "Predicting Personalized Image Emotion Perceptions in Social Networks",
            "why": "Highly cited (168)."
          },
          {
            "title": "Affective Image Content Analysis: Two Decades Review and New Perspectives",
            "why": "Highly cited (157)."
          },
          {
            "title": "Personality-Assisted Multi-Task Learning for Generic and Personalized Image Aesthetics Assessment",
            "why": "Highly cited (153)."
          },
          {
            "title": "Generating virtual ratings from chinese reviews to augment online recommendations",
            "why": "Highly cited (86)."
          },
          {
            "title": "Affective Image Content Analysis: A Comprehensive Survey",
            "why": "Highly cited (81)."
          },
          {
            "title": "Personalized Emotion Recognition by Personality-Aware High-Order Learning of Physiological Signals",
            "why": "Highly cited (78)."
          },
          {
            "title": "EmotionGAN",
            "why": "Highly cited (75)."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 278,
        "interesting_works_count": 23,
        "new_works_count": 0,
        "deduped_works_count": 278,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5057732142.json"
    },
    {
      "identity": {
        "name": "Guoying Zhao",
        "orcid": "https://orcid.org/0000-0003-3694-206X",
        "google_scholar": "http://scholar.google.com/citations?user=hzywrFMAAAAJ",
        "openalex_author_id": "A5082301986",
        "openalex_author_url": "https://openalex.org/A5082301986"
      },
      "affiliation": {
        "last_known_institution": "University of Oulu",
        "last_known_country": "Finland",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 599,
        "cited_by_count": 29022,
        "h_index": 85,
        "i10_index": 283
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Micro-expression Recognition",
            "weight": 0.95
          },
          {
            "name": "Facial Expression Analysis",
            "weight": 0.9
          },
          {
            "name": "Multimodal Emotion Recognition",
            "weight": 0.85
          },
          {
            "name": "Physiological Signal Processing",
            "weight": 0.8
          },
          {
            "name": "Affective Computing",
            "weight": 0.75
          },
          {
            "name": "Domain Adaptation",
            "weight": 0.6
          },
          {
            "name": "Deep Learning for Emotion",
            "weight": 0.55
          },
          {
            "name": "Responsible Affective Computing",
            "weight": 0.4
          }
        ],
        "trend_summary": "Research from 2007-2011 established foundational dynamic texture recognition and facial expression analysis using local binary patterns. During 2012-2016, focus shifted significantly toward spontaneous micro-expression recognition, marked by the creation of benchmark datasets like CASME II and spatiotemporal descriptor refinement. The 2017-2021 period expanded scope to multimodal emotion recognition and physiological signal processing, notably remote photoplethysmography (rPPG) and cross-domain adaptation using deep learning architectures. Recent work from 2022-2025 integrates large language models, generative AI, and responsible affective computing, addressing privacy, ethics, and complex real-world applications such as dementia monitoring and educational analytics. Current efforts prioritize robustness in dynamic domain shifts, self-supervised learning, and the fusion of vision-language models for fine-grained emotion understanding, reflecting a transition from isolated visual cues to holistic, ethically-aware multimodal systems.",
        "representative_papers": [
          {
            "title": "Dynamic Texture Recognition Using Local Binary Patterns with an Application to Facial Expressions",
            "why": "Foundational work establishing LBP-based dynamic texture analysis for emotion recognition in the earliest window."
          },
          {
            "title": "CASME II: An Improved Spontaneous Micro-Expression Database and the Baseline Evaluation",
            "why": "Critical dataset release that standardized spontaneous micro-expression analysis during the 2012-2016 period."
          },
          {
            "title": "A Main Directional Mean Optical Flow Feature for Spontaneous Micro-Expression Recognition",
            "why": "Highly cited method defining feature extraction techniques for micro-expressions in the mid-phase."
          },
          {
            "title": "Remote Photoplethysmograph Signal Measurement from Facial Videos Using Spatio-Temporal Networks",
            "why": "Marks the strategic shift towards non-contact physiological signal processing for affective computing."
          },
          {
            "title": "Video-Based Remote Physiological Measurement via Cross-Verified Feature Disentangling",
            "why": "Represents the peak of rPPG research integration with deep learning and cross-domain adaptation."
          },
          {
            "title": "From Emotion AI to Cognitive AI",
            "why": "Signifies the conceptual transition from pure emotion recognition to broader cognitive and explainable AI."
          },
          {
            "title": "MER 2025: When Affective Computing Meets Large Language Models",
            "why": "Illustrates the latest integration of generative AI and LLMs into emotion understanding frameworks."
          },
          {
            "title": "3-D Face De-Identification With Preserving Multi-Facial Attributes: A Benchmark",
            "why": "Highlights the emerging focus on privacy preservation and responsible AI in facial analysis."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 522,
        "interesting_works_count": 168,
        "new_works_count": 72,
        "deduped_works_count": 522,
        "ai_called_count": 72,
        "summary_recomputed": true
      },
      "profile_path": "data/researchers/profiles/A5082301986.json"
    },
    {
      "identity": {
        "name": "Honghai Liu",
        "orcid": "https://orcid.org/0000-0002-2880-4698",
        "google_scholar": "https://scholar.google.com/citations?user=bsabUhgAAAAJ",
        "openalex_author_id": "A5085867312",
        "openalex_author_url": "https://openalex.org/A5085867312"
      },
      "affiliation": {
        "last_known_institution": "University of Portsmouth",
        "last_known_country": "United Kingdom",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 793,
        "cited_by_count": 14756,
        "h_index": 64,
        "i10_index": 277
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective computing in neurodevelopmental disorders (especially ASD)",
            "weight": 1
          },
          {
            "name": "Multimodal emotion analysis (vision, language, physiology, audio, EEG, eye-tracking)",
            "weight": 0.97
          },
          {
            "name": "Emotion-aware AI and affective human-computer/robot interaction",
            "weight": 0.94
          },
          {
            "name": "Affective biomarker discovery and clinical behavioral phenotyping",
            "weight": 0.91
          },
          {
            "name": "Robust affective signal processing (noisy labels, occlusion, pose, low-res, cross-subject generalization)",
            "weight": 0.88
          },
          {
            "name": "Interpretable and psychologically-informed affective modeling",
            "weight": 0.85
          },
          {
            "name": "Affective neuroscience and neurophysiological emotion decoding (EEG, ECG, cortico-ocular coupling)",
            "weight": 0.82
          },
          {
            "name": "Emotion-aware adaptive systems for rehabilitation, therapy, and mental health support",
            "weight": 0.79
          }
        ],
        "trend_summary": "Honghai Liu's research centers on clinically grounded, multimodal affective computing—particularly for autism spectrum disorder (ASD) and related neurodevelopmental conditions. His work bridges AI (vision-language models, robust representation learning, interpretable deep learning), clinical psychology (caregiver stress, self-compassion, empathy, parenting), and neurophysiology (EEG, eye-tracking, sEMG, ECG) to develop validated affective biomarkers, assistive technologies (robotic therapy, screening systems), and ethically informed emotion-aware interfaces. A strong longitudinal thread is the translation of computational affective science into real-world clinical tools—from early toddler screening (e.g., Response-to-Name) to fatigue-adaptive training and compassion-targeted interventions.",
        "representative_papers": [
          {
            "title": "Classifying ASD children with LSTM based on raw videos",
            "why": "Seminal early work demonstrating deep learning for raw-video-based behavioral phenotyping in ASD—foundational for his clinical multimodal pipeline."
          },
          {
            "title": "Robot-Enhanced Therapy: Development and Validation of Supervised Autonomous Robotic System for Autism Spectrum Disorders Therapy",
            "why": "Landmark paper establishing his integrative framework for affective human-robot interaction in clinical intervention, highly cited and conceptually influential."
          },
          {
            "title": "Deep EEG Superresolution via Correlating Brain Structural and Functional Connectivities",
            "why": "Exemplifies his cross-modal neurocomputational approach—merging structural and functional neuroimaging to enhance affective signal fidelity in EEG-based emotion decoding."
          },
          {
            "title": "SNEFER: Stopping the Negative Effect of Noisy Labels Adaptively in Facial Expression Recognition",
            "why": "Represents his leadership in robust affective AI—addressing real-world annotation noise, a critical bottleneck in clinical emotion datasets."
          },
          {
            "title": "Computational Interpersonal Communication Model for Screening Autistic Toddlers: A Case Study of Response-to-Name",
            "why": "Embodies his translational focus: building theory-driven, interpretable, and clinically deployable computational models for early ASD detection."
          },
          {
            "title": "Self-compassion, mental health, and parenting: Comparing parents of autistic and non-autistic children",
            "why": "Highlights his expansion into caregiver affective mechanisms—linking emotion regulation constructs (self-compassion) to family-system outcomes, bridging clinical psychology and AI-informed intervention design."
          },
          {
            "title": "Cortico-Ocular Coupling Analysis for Developmental and Behavioral Disorders: A Review",
            "why": "Synthesizes his emerging direction in multimodal neurophysiological biomarkers—unifying oculomotor and cortical dynamics as emotion-linked attentional signatures in atypical development."
          },
          {
            "title": "Text Prompt Region Decomposition for Effective Facial Expression Recognition",
            "why": "Illustrates his cutting-edge integration of vision-language foundation models with affective computing—enabling fine-grained, interpretable, and occlusion-robust emotion analysis using text-guided region decomposition."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 732,
        "interesting_works_count": 65,
        "new_works_count": 0,
        "deduped_works_count": 732,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5085867312.json"
    },
    {
      "identity": {
        "name": "Hongxun Yao",
        "orcid": "https://orcid.org/0000-0003-3298-2574",
        "google_scholar": "https://scholar.google.com/citations?user=aOMFNFsAAAAJ",
        "openalex_author_id": "A5023274785",
        "openalex_author_url": "https://openalex.org/A5023274785"
      },
      "affiliation": {
        "last_known_institution": "China Mobile (China)",
        "last_known_country": "China",
        "source": "openalex"
      },
      "metrics": {
        "works_count": 417,
        "cited_by_count": 11216,
        "h_index": 46,
        "i10_index": 153
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective image analysis",
            "weight": 1
          },
          {
            "name": "Emotion-aware computer vision",
            "weight": 0.97
          },
          {
            "name": "Multimodal emotion recognition",
            "weight": 0.93
          },
          {
            "name": "Human-centered affective AI",
            "weight": 0.9
          },
          {
            "name": "Probabilistic emotion modeling",
            "weight": 0.86
          },
          {
            "name": "Affective domain adaptation",
            "weight": 0.82
          },
          {
            "name": "Emotion dynamics and adaptation",
            "weight": 0.78
          },
          {
            "name": "Interpretability and fuzzy modeling in affective systems",
            "weight": 0.74
          }
        ],
        "trend_summary": "Hongxun Yao's research spans over two decades, evolving from foundational work in affective image analysis and facial expression recognition (2002–2015) toward increasingly sophisticated, human-centered, and cognitively grounded paradigms. Early contributions pioneered probabilistic, sparse, and distributional modeling of subjective image emotions; later work expanded into multimodal fusion (speech, image, video, social context), adaptive and dynamic emotion modeling, and robust learning under data scarcity. Recent directions (2024–2025) emphasize cross-domain generalization (e.g., CLIP-based and prompt-guided emotion adaptation), disentangled and interpretable representations (e.g., via information bottleneck and fuzzy systems), and emotion-aware generative modeling — all unified by a strong commitment to psychological plausibility, user subjectivity, and real-world applicability in social media, multimedia, and HCI.",
        "representative_papers": [
          {
            "title": "Predicting Personalized Image Emotion Perceptions in Social Networks",
            "why": "Highly cited (168), foundational for human-centered, subjective, and socially contextualized affective modeling; introduced personalized emotion perception grounded in social network data."
          },
          {
            "title": "Continuous Probability Distribution Prediction of Image Emotions via Multitask Shared Sparse Regression",
            "why": "Seminal (191 citations) in probabilistic affective computing; established continuous valence-arousal prediction with multitask sparse learning — core methodology influencing later distributional approaches."
          },
          {
            "title": "Exploring Principles-of-Art Features For Image Emotion Recognition",
            "why": "Most cited paper (355), bridging computational aesthetics and emotion; introduced art-theoretic visual features for affective vision — highly influential in cross-disciplinary affective image analysis."
          },
          {
            "title": "Emotion in a Bottle: Information Bottleneck Guided Disentanglement for Emotion Domain Adaptation",
            "why": "Represents cutting-edge direction: integrates information bottleneck theory with disentanglement for domain-adaptive affective modeling — key for generalizable, interpretable emotion AI."
          },
          {
            "title": "Soul Dancer",
            "why": "Pioneering work on emotion-aware generative modeling and multimodal emotional motion synthesis — early demonstration of affective semantics in generative AI."
          },
          {
            "title": "Affective Image Retrieval via Multi-Graph Learning",
            "why": "Influential (116 citations) in emotion-aware multimedia retrieval; introduced graph-based fusion of heterogeneous affective features — foundational for modern multimodal affective search."
          },
          {
            "title": "Guest Editorial: Special Issue on Fuzzy Affective Computing Systems",
            "why": "Signals strategic shift toward interpretability and uncertainty modeling in affective AI; highlights integration of fuzzy logic for human-aligned, explainable emotion reasoning."
          },
          {
            "title": "Emotional Conflict Adaptation for Multimodal Sentiment Analysis",
            "why": "Exemplifies recent focus on emotion dynamics — models intra- and inter-modal emotional conflict as a driver for adaptive sentiment understanding, reflecting cognitive realism."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 398,
        "interesting_works_count": 31,
        "new_works_count": 0,
        "deduped_works_count": 398,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5023274785.json"
    },
    {
      "identity": {
        "name": "Jia Liu",
        "orcid": "https://orcid.org/0000-0003-0383-0934",
        "google_scholar": "https://scholar.google.com/citations?user=xPoVpSEAAAAJ",
        "openalex_author_id": "A5100409741",
        "openalex_author_url": "https://openalex.org/A5100409741"
      },
      "affiliation": {
        "last_known_institution": "Tsinghua University",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 1245,
        "cited_by_count": 36407,
        "h_index": 71,
        "i10_index": 391
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective Neuroscience",
            "weight": 1
          },
          {
            "name": "Emotion-Cognition Interaction",
            "weight": 0.9
          },
          {
            "name": "Clinical Affective Science",
            "weight": 0.8
          },
          {
            "name": "Individual Differences in Emotion",
            "weight": 0.7
          },
          {
            "name": "Neural Biomarkers of Affect",
            "weight": 0.6
          },
          {
            "name": "Social Emotion Processing",
            "weight": 0.5
          },
          {
            "name": "Emotion Regulation Mechanisms",
            "weight": 0.4
          },
          {
            "name": "Affective Computing and AI",
            "weight": 0.3
          }
        ],
        "trend_summary": "Research began (2002-2006) with foundational work on face perception stages and clinical psychology aspects of anxiety and hypertension, though evidence is limited for this window. During 2007-2011, focus expanded to affective computing in speech and neural correlates of insight and trait anxiety using ERP and fMRI. The 2012-2016 period marked a peak in affective neuroscience, extensively utilizing VBM and resting-state fMRI to map structural and functional biomarkers of depression, PTSD, mindfulness, and self-processing. From 2017-2021, studies maintained neural mechanistic inquiries while integrating environmental factors like parental care and pandemic-related stress, emphasizing resilience and intervention. Currently (2022-2026), the focus has evolved towards applied affective science, examining AI emotional intelligence, ESG sentiment, educational support, and cross-cultural emotion processing, alongside continued investigation into neural biomarkers of psychopathology and recovery.",
        "representative_papers": [
          {
            "title": "Stages of processing in face perception: an MEG study",
            "why": "Foundational high-impact work establishing temporal dynamics of emotional face perception."
          },
          {
            "title": "Neural correlates of psychological resilience and their relation to life satisfaction in a sample of healthy young adults",
            "why": "Defines neural mechanisms linking psychological resilience to life satisfaction."
          },
          {
            "title": "Frequency-selective control of cortical and subcortical networks by central thalamus",
            "why": "Elucidates frequency-dependent neuromodulation of affective states."
          },
          {
            "title": "Altered resting-state functional activity in posttraumatic stress disorder: A quantitative meta-analysis",
            "why": "Comprehensive meta-analysis mapping affective circuit dysfunction in trauma."
          },
          {
            "title": "Brain grey matter volume alterations in late-life depression",
            "why": "Identifies structural biomarkers specific to geriatric affective disorders."
          },
          {
            "title": "Emotional intelligence of Large Language Models",
            "why": "Pivotal shift towards evaluating emotional intelligence in artificial agents."
          },
          {
            "title": "The neural mechanisms of explicit and implicit processing of Chinese emotion-label and emotion-laden words: evidence from emotional categorisation and emotional Stroop tasks",
            "why": "Advances cross-linguistic affective semantics and neural word processing."
          },
          {
            "title": "Central symptoms and network associations of depressive symptoms among school-aged students: A network analysis",
            "why": "Applies network analysis to model affective symptom structures in youth."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 1161,
        "interesting_works_count": 121,
        "new_works_count": 6,
        "deduped_works_count": 1161,
        "ai_called_count": 6,
        "summary_recomputed": true
      },
      "profile_path": "data/researchers/profiles/A5100409741.json"
    },
    {
      "identity": {
        "name": "Jianhua Tao",
        "orcid": "https://orcid.org/0000-0002-9344-6428",
        "google_scholar": null,
        "openalex_author_id": "A5112613657",
        "openalex_author_url": "https://openalex.org/A5112613657"
      },
      "affiliation": {
        "last_known_institution": "Tsinghua University",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 617,
        "cited_by_count": 8562,
        "h_index": 43,
        "i10_index": 188
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective multimodal fusion",
            "weight": 1
          },
          {
            "name": "Emotion-personality modeling",
            "weight": 0.97
          },
          {
            "name": "Multimodal emotion recognition",
            "weight": 0.94
          },
          {
            "name": "Affective speech processing",
            "weight": 0.91
          },
          {
            "name": "Clinical affective computing",
            "weight": 0.88
          },
          {
            "name": "Self-supervised representation learning for affect",
            "weight": 0.85
          },
          {
            "name": "Affective speech synthesis",
            "weight": 0.82
          },
          {
            "name": "Large language models for affect",
            "weight": 0.79
          }
        ],
        "trend_summary": "Jianhua Tao's research centers on foundational and applied affective AI, with a strong emphasis on multimodal integration—especially audio-visual-text fusion for sentiment and emotion analysis. A defining theme is the tight coupling of personality and emotion, treating them as interdependent psychological constructs rather than isolated traits. Recent work increasingly leverages self-supervised learning (e.g., masked autoencoders, contrastive learning) to overcome annotation scarcity, while also pioneering LLM-augmented affective reasoning and zero-shot emotion understanding. Clinical translation—particularly depression detection from speech and facial dynamics—is a sustained, high-impact thread. The trajectory shows a clear evolution from traditional feature engineering and modality-specific models toward unified, robust, psychologically grounded, and human-centered multimodal architectures.",
        "representative_papers": [
          {
            "title": "HiCMAE: Hierarchical Contrastive Masked Autoencoder for self-supervised Audio-Visual Emotion Recognition",
            "why": "Exemplifies the shift toward scalable, label-efficient self-supervised learning for multimodal affect, achieving top citation count (42) in 2024 and establishing a new paradigm for cross-modal representation learning."
          },
          {
            "title": "Efficient Multimodal Transformer With Dual-Level Feature Restoration for Robust Multimodal Sentiment Analysis",
            "why": "Highly influential (190 citations), it addresses core challenges in real-world deployment—missing modalities and noise—via innovative feature restoration, defining robustness as a first-class design principle in affective fusion."
          },
          {
            "title": "DepressionMLP: A Multi-Layer Perceptron Architecture for Automatic Depression Level Prediction via Facial Keypoints and Action Units",
            "why": "Represents the clinical translation pillar: interpretable, behaviorally grounded modeling (22 citations) using accessible visual biomarkers, bridging computational affect and mental health practice."
          },
          {
            "title": "PSA-MF: Personality-Sentiment Aligned Multi-Level Fusion for Multimodal Sentiment Analysis",
            "why": "Captures the signature personality-affect integration theme—explicitly aligning personality traits with sentiment dynamics across modalities, a conceptual and architectural innovation in 2025."
          },
          {
            "title": "GPT-4V with emotion: A zero-shot benchmark for Generalized Emotion Recognition",
            "why": "Signals strategic engagement with foundation models—benchmarking emergent zero-shot capabilities of VLMs for affect, highlighting scalability and generalization beyond narrow datasets (55 citations)."
          },
          {
            "title": "MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic Facial Expression Recognition",
            "why": "Landmark self-supervised work (55 citations) demonstrating that masked reconstruction can effectively capture fine-grained temporal facial dynamics without labels—key for micro-expression and real-world use."
          },
          {
            "title": "ICaps-ResLSTM: Improved capsule network and residual LSTM for EEG emotion recognition",
            "why": "Highlights neurophysiological depth and methodological diversity—integrating capsule networks with RNNs for brain signal-based emotion decoding (58 citations), expanding multimodality to neural interfaces."
          },
          {
            "title": "CHEAVD: a Chinese natural emotional audio–visual database",
            "why": "Foundational resource (107 citations) enabling culturally grounded, naturalistic affective AI research—underscoring Tao’s long-standing commitment to high-quality, ecologically valid multimodal data infrastructure."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 540,
        "interesting_works_count": 218,
        "new_works_count": 0,
        "deduped_works_count": 540,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5112613657.json"
    },
    {
      "identity": {
        "name": "Jufeng Yang",
        "orcid": "https://orcid.org/0000-0003-0219-3443",
        "google_scholar": "https://scholar.google.com/citations?user=c5vDJv0AAAAJ",
        "openalex_author_id": "A5089409678",
        "openalex_author_url": "https://openalex.org/A5089409678"
      },
      "affiliation": {
        "last_known_institution": "Nankai University",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 147,
        "cited_by_count": 5716,
        "h_index": 33,
        "i10_index": 76
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "multimodal emotion recognition",
            "weight": 1
          },
          {
            "name": "affective computing",
            "weight": 0.98
          },
          {
            "name": "weakly supervised affective modeling",
            "weight": 0.95
          },
          {
            "name": "emotion-aware AI systems",
            "weight": 0.92
          },
          {
            "name": "affective multimodal AI",
            "weight": 0.89
          },
          {
            "name": "robust emotion-aware mental health assessment",
            "weight": 0.86
          },
          {
            "name": "fairness and bias mitigation in affective AI",
            "weight": 0.83
          },
          {
            "name": "low-resource and label-efficient affective learning",
            "weight": 0.8
          }
        ],
        "trend_summary": "Jufeng Yang's research centers on advancing affective AI through multimodal (audio-visual-text) integration, with strong emphasis on robustness, efficiency, and human-centered design. Key trends include: (1) shifting from categorical to dimensional/continuous, distributional, and ambiguous emotion modeling; (2) addressing real-world constraints via weak, semi-, zero-, and low-shot supervision; (3) integrating fairness, bias mitigation, and human-aligned evaluation; (4) expanding applications to mental health (e.g., depression detection), digital humans, tourism, and social media; and (5) grounding models in cognitive-affective principles and biomechanical signals (e.g., gait). The work consistently bridges computer vision, NLP, and signal processing within an affective computing framework.",
        "representative_papers": [
          {
            "title": "Multimodal Sentiment Analysis With Image-Text Interaction Network",
            "why": "Highly cited foundational paper (194 citations) establishing core image-text interaction paradigms for multimodal sentiment — exemplifies early leadership in affective multimodal fusion."
          },
          {
            "title": "Affective Image Content Analysis: Two Decades Review and New Perspectives",
            "why": "Influential survey (157 citations) framing long-term research vision, highlighting noisy labels, personalization, and group emotion — anchors theoretical and methodological evolution in visual affect."
          },
          {
            "title": "DIP: Dual Incongruity Perceiving Network for Sarcasm Detection",
            "why": "Landmark work (54 citations) introducing affective incongruity modeling for sarcasm — demonstrates deep integration of emotion semantics into pragmatic language understanding."
          },
          {
            "title": "READ-Net: Clarifying Emotional Ambiguity via Adaptive Feature Recalibration for Audio-Visual Depression Detection",
            "why": "Represents cutting-edge clinical translation — targets depression detection using adaptive recalibration of ambiguous affective signals across modalities."
          },
          {
            "title": "BOVIS: Bias-Mitigated Object-Enhanced Visual Emotion Analysis",
            "why": "Exemplifies growing focus on fairness — introduces object-aware bias mitigation, bridging interpretability, equity, and visual emotion understanding."
          },
          {
            "title": "Toward Label-Efficient Emotion and Sentiment Analysis",
            "why": "Seminal contribution (34 citations) to data-efficient affective AI — systematically advances weak/semi-supervised paradigms critical for real-world deployment."
          },
          {
            "title": "Looking Into Gait for Perceiving Emotions via Bilateral Posture and Movement Graph Convolutional Networks",
            "why": "Highlights expansion beyond face/voice — pioneers biomechanically grounded affect perception using graph neural networks on gait, enabling non-intrusive emotion sensing."
          },
          {
            "title": "MART: Masked Affective RepresenTation Learning via Masked Temporal Distribution Distillation",
            "why": "Embodies modern representation learning direction — applies masked distillation to temporal affective distributions, aligning with foundation-model-inspired affective pretraining."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 133,
        "interesting_works_count": 49,
        "new_works_count": 0,
        "deduped_works_count": 133,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5089409678.json"
    },
    {
      "identity": {
        "name": "Kejun Zhang",
        "orcid": null,
        "google_scholar": "https://scholar.google.com/citations?user=0t1B3vMAAAAJ",
        "openalex_author_id": "A5101577175",
        "openalex_author_url": "https://openalex.org/A5101577175"
      },
      "affiliation": {
        "last_known_institution": "University of Science and Technology of China",
        "last_known_country": "China",
        "source": "openalex"
      },
      "metrics": {
        "works_count": 182,
        "cited_by_count": 1955,
        "h_index": 18,
        "i10_index": 32
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective multimodal emotion analysis",
            "weight": 1
          },
          {
            "name": "Emotion-aware generative AI (music/audiovisual)",
            "weight": 0.97
          },
          {
            "name": "Affective human–animal/robot–human interaction",
            "weight": 0.93
          },
          {
            "name": "Emotion-sensitive multimedia systems (real-time, context-aware)",
            "weight": 0.91
          },
          {
            "name": "Therapeutic and wellness-oriented affective co-creation",
            "weight": 0.88
          },
          {
            "name": "Cross-modal affective modeling (music–vision–physiology–language)",
            "weight": 0.85
          },
          {
            "name": "Affective recommender systems (mood-, context-, culture-aware)",
            "weight": 0.82
          },
          {
            "name": "Psychology-informed HCI for emotional well-being",
            "weight": 0.79
          }
        ],
        "trend_summary": "Kejun Zhang's research centers on affective computing with strong emphasis on multimodal emotion sensing, modeling, and generative application—especially at the intersection of music, video, physiology (e.g., EDA, EEG), and expressive language. A defining trend is the shift from passive emotion recognition toward active, real-time, context- and relationship-aware affective synthesis: e.g., AI-assisted human-pet musical co-creation, emotion-conditioned video-to-music generation, and wearables-powered pet communication. There is consistent attention to applied domains—wellness therapy, retail robotics, creative tools—and deep integration of psychological principles (e.g., emotional flow, synesthesia, memory resonance) into system design. Recent work increasingly unifies LLMs, diffusion models, and physiological signals within human-centered, cross-species, and therapeutic frameworks.",
        "representative_papers": [
          {
            "title": "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals",
            "why": "Highly cited foundational work demonstrating early integration of physiological and audio modalities for robust emotion modeling—core to Zhang's multimodal affect paradigm."
          },
          {
            "title": "Diff-V2M: A Hierarchical Conditional Diffusion Model with Explicit Rhythmic Modeling for Video-to-Music Generation",
            "why": "Represents cutting-edge direction: state-of-the-art generative modeling explicitly embedding rhythmic and affective alignment between visual and auditory modalities."
          },
          {
            "title": "PetChat: An Emotion-Aware Pet Communication System Powered by LLMs and Wearable Devices",
            "why": "Exemplifies novel human-animal affective interaction paradigm, combining wearables, LLMs, and real-time emotion inference—expanding affective computing beyond human-centric boundaries."
          },
          {
            "title": "AI-Assisted Human-Pet Artistic Musical Co-Creation for Wellness Therapy",
            "why": "Bridges therapeutic application, interspecies affect, and generative music—highlighting Zhang’s focus on emotionally grounded, participatory wellness technologies."
          },
          {
            "title": "REMAST: Real-Time Emotion-Based Music Arrangement With Soft Transition",
            "why": "Key representative of real-time, adaptive affective multimedia systems—emphasizing smooth emotional continuity in interactive soundscapes."
          },
          {
            "title": "SoundScape: A Human-AI Co-Creation System Making Your Memories Heard",
            "why": "Embodies psychology-informed, memory-augmented affective expression—integrating narrative, emotion, and sonic design for personal meaning-making."
          },
          {
            "title": "Consumer shopping emotion and interest database: a unique database with a multimodal emotion recognition method for retail service robots to infer consumer shopping intentions better than humans",
            "why": "Landmark applied work showing real-world impact: multimodal affect inference enabling robots to outperform humans in intention prediction—bridging lab research and service AI."
          },
          {
            "title": "The PMEmo Dataset for Music Emotion Recognition",
            "why": "Foundational resource paper with highest citation count; enabled reproducible, physiology-grounded music emotion research—cornerstone of Zhang’s long-term affective MIR agenda."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 163,
        "interesting_works_count": 35,
        "new_works_count": 0,
        "deduped_works_count": 163,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5101577175.json"
    },
    {
      "identity": {
        "name": "Leida Li",
        "orcid": "https://orcid.org/0000-0001-9069-8796",
        "google_scholar": "https://scholar.google.com/citations?user=xMvuFI8AAAAJ",
        "openalex_author_id": "A5033615240",
        "openalex_author_url": "https://openalex.org/A5033615240"
      },
      "affiliation": {
        "last_known_institution": "Xidian University",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 310,
        "cited_by_count": 7168,
        "h_index": 44,
        "i10_index": 153
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective computing",
            "weight": 1
          },
          {
            "name": "Emotion-aware AI",
            "weight": 0.95
          },
          {
            "name": "Multimodal emotion perception",
            "weight": 0.92
          },
          {
            "name": "Computational aesthetics",
            "weight": 0.88
          },
          {
            "name": "Human-centered AI evaluation",
            "weight": 0.85
          },
          {
            "name": "Cross-modal affective alignment",
            "weight": 0.82
          },
          {
            "name": "Psychologically grounded machine learning",
            "weight": 0.78
          },
          {
            "name": "Personalized aesthetic modeling",
            "weight": 0.75
          }
        ],
        "trend_summary": "Leida Li's research centers on bridging affective science and AI—specifically, modeling subjective human perceptions (aesthetics, emotion, personality) through multimodal, psychologically informed, and human-centered computational frameworks. A strong longitudinal trend emerges: evolving from unimodal emotion/sentiment analysis (2019–2022) toward foundation-model-ready, vision-language-affective integration (2023–2024), with increasing emphasis on neuro-inspired architectures, expert-curated benchmarks (e.g., AesBench), explainability, and empathy-aligned evaluation. Cross-cutting themes include subjective trait extraction, culture- and personality-aware personalization, and rigorous grounding in affective psychology and neuroscience.",
        "representative_papers": [
          {
            "title": "Multimodal Sentiment Analysis With Image-Text Interaction Network",
            "why": "Highly cited foundational work (194 citations) establishing cross-modal affective fusion for sentiment, setting early methodological precedent for later aesthetics-emotion integration."
          },
          {
            "title": "SOLVER: Scene-Object Interrelated Visual Emotion Reasoning Network",
            "why": "Seminal graph-based, psychologically inspired model for explainable visual emotion reasoning—introduces structured affective reasoning now extended to aesthetics assessment."
          },
          {
            "title": "BMI-Net: A Brain-inspired Multimodal Interaction Network for Image Aesthetic Assessment",
            "why": "Key neuro-inspired architecture linking brain mechanisms to subjective aesthetic judgment, exemplifying the convergence of neuroscience and computational aesthetics."
          },
          {
            "title": "AesExpert: Towards Multi-modality Foundation Model for Image Aesthetics Perception",
            "why": "Flagship 2024 paper defining the shift toward large-scale, expert-validated multimodal foundation models for aesthetics, integrating affective and perceptual priors."
          },
          {
            "title": "AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception",
            "why": "Introduces the first expert-curated benchmark explicitly designed to evaluate affective understanding in multimodal LLMs—critical for human-AI empathy alignment."
          },
          {
            "title": "Emotion-aware hierarchical interaction network for multimodal image aesthetics assessment",
            "why": "Exemplifies the core paradigm: jointly modeling aesthetics and emotion via hierarchical, multimodal interaction—directly unifying two central directions."
          },
          {
            "title": "Psychology Inspired Model for Hierarchical Image Aesthetic Attribute Prediction",
            "why": "Early demonstration of interpretability through affective psychology, laying groundwork for psychologically grounded representation learning in later works."
          },
          {
            "title": "Personality-Assisted Multi-Task Learning for Generic and Personalized Image Aesthetics Assessment",
            "why": "Pivotal in establishing personality as a key modulator of aesthetic perception—enabling personalized, affectively aware multimedia systems."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 291,
        "interesting_works_count": 16,
        "new_works_count": 0,
        "deduped_works_count": 291,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5033615240.json"
    },
    {
      "identity": {
        "name": "Meng Wang",
        "orcid": "https://orcid.org/0000-0002-3094-7735",
        "google_scholar": "https://scholar.google.com/citations?user=rHagaaIAAAAJ",
        "openalex_author_id": "A5100377147",
        "openalex_author_url": "https://openalex.org/A5100377147"
      },
      "affiliation": {
        "last_known_institution": "Hefei University of Technology",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 1461,
        "cited_by_count": 41979,
        "h_index": 103,
        "i10_index": 615
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective multimodal learning",
            "weight": 1
          },
          {
            "name": "Clinical emotion modeling",
            "weight": 0.97
          },
          {
            "name": "Robust and generalizable emotion recognition",
            "weight": 0.94
          },
          {
            "name": "Emotion-aware dialogue and engagement systems",
            "weight": 0.91
          },
          {
            "name": "Affective signal processing from video and physiology",
            "weight": 0.88
          },
          {
            "name": "LLM-augmented affective modeling",
            "weight": 0.85
          },
          {
            "name": "Developmental and translational affective neuroscience",
            "weight": 0.82
          },
          {
            "name": "Explainable and interpretable affective AI",
            "weight": 0.79
          }
        ],
        "trend_summary": "Meng Wang's research centers on human-centered affective AI, with strong emphasis on multimodal integration (vision, audio, text, physiology) for robust, clinically grounded, and interpretable emotion understanding. A clear trend is the convergence of deep learning, clinical psychology, and computational neuroscience—especially in mental health screening, developmental affective trajectories, and real-world deployment (e.g., interviews, dialogue, education). Recent work increasingly leverages LLMs for psychological grounding, incorporates biological (genetic, epigenetic, neurophysiological) priors, and prioritizes generalization across domains, populations, and noisy 'in-the-wild' conditions. There is also sustained focus on fine-grained behavioral analysis (micro-actions, dynamic expressions) and emotion-aware generative tasks (captioning, dialogue, animation).",
        "representative_papers": [
          {
            "title": "Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark",
            "why": "Exemplifies the shift from static recognition to dynamic, reasoning-based, multi-turn affective understanding—integrating dialogue, multimodality, and cognitive modeling."
          },
          {
            "title": "Multimodal Depression Estimation via Contrastive Modality Alignment and Fusion",
            "why": "Represents core clinical affective modeling: bridging multimodal signals (e.g., video, audio, text) with contrastive learning for real-world mental health assessment."
          },
          {
            "title": "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization",
            "why": "Highlights the strong focus on robustness and generalization—addressing domain shift, label noise, and population heterogeneity using distributionally robust optimization."
          },
          {
            "title": "VAEmo: Efficient Representation Learning for Visual-Audio Emotion With Knowledge Injection",
            "why": "Embodies the fusion of multimodal learning with knowledge injection (e.g., psychological constructs or LLM-derived priors) for low-resource, semantically grounded emotion representation."
          },
          {
            "title": "Generalizable Engagement Estimation in Conversation via Domain Prompting and Parallel Attention",
            "why": "Illustrates the integration of affective modeling into dialogue systems using prompt-based transfer and attention mechanisms tailored for cross-domain engagement estimation."
          },
          {
            "title": "Temporal Gated Face Alignment Network for Camera-Based Physiological Sensing",
            "why": "Demonstrates innovation in contactless affective signal processing—linking facial dynamics to autonomic physiology for unobtrusive mental state monitoring."
          },
          {
            "title": "Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors",
            "why": "Captures the emerging synergy between LLMs and affective science—using psychologically structured LLM representations to ground multimodal trait and emotion inference."
          },
          {
            "title": "Evaluation of environmental-genetic factors and mental health outcomes for sleep disturbance from late childhood to early adolescence",
            "why": "Reflects the translational, developmental neuroscience direction—modeling gene-environment interactions in affective vulnerability across critical developmental windows."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 1323,
        "interesting_works_count": 85,
        "new_works_count": 0,
        "deduped_works_count": 1323,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5100377147.json"
    },
    {
      "identity": {
        "name": "Ming Li",
        "orcid": "https://orcid.org/0000-0002-6406-1983",
        "google_scholar": "https://scholar.google.com/citations?user=zp2Kz44AAAAJ",
        "openalex_author_id": "A5100351449",
        "openalex_author_url": "https://openalex.org/A5100351449"
      },
      "affiliation": {
        "last_known_institution": "Chinese University of Hong Kong, Shenzhen",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 403,
        "cited_by_count": 8134,
        "h_index": 37,
        "i10_index": 116
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective computing for autism interventions",
            "weight": 1
          },
          {
            "name": "Emotion recognition from neural signals",
            "weight": 0.97
          },
          {
            "name": "Affective speech synthesis and paralinguistic emotion analysis",
            "weight": 0.94
          },
          {
            "name": "Multimodal emotion sensing and recognition",
            "weight": 0.91
          },
          {
            "name": "Clinical affective computing for neurodevelopmental disorders",
            "weight": 0.88
          },
          {
            "name": "Robust emotion modeling under annotation subjectivity and domain shift",
            "weight": 0.85
          },
          {
            "name": "Affective human-computer interaction in clinical VR and digital therapeutics",
            "weight": 0.82
          },
          {
            "name": "Emotion-aware multimodal AI for clinical assessment",
            "weight": 0.79
          }
        ],
        "trend_summary": "Ming Li's research centers on clinically grounded, multimodal affective computing—especially bridging emotion science with neurodevelopmental conditions (notably autism spectrum disorder). A strong longitudinal thread spans EEG-based emotion decoding, expressive speech modeling (including non-verbal cues and escalation detection), and behaviorally anchored digital interventions (e.g., VR games, collaborative turn-taking systems). Recent work emphasizes robustness—handling inconsistent labels, cross-domain adaptation, and inclusive modeling for atypical populations—while increasingly integrating LLMs, pre-trained vision-language models, and corpus development for affective AI. The trajectory shows a maturing integration of neuroscience, signal processing, clinical psychology, and human-centered AI design.",
        "representative_papers": [
          {
            "title": "StarRescue: the Design and Evaluation of A Turn-Taking Collaborative Game for Facilitating Autistic Children's Social Skills",
            "why": "Exemplifies applied, intervention-oriented affective computing—co-designing and empirically evaluating a game that embeds socio-emotional scaffolding and turn-taking as regulatory mechanisms for autistic children."
          },
          {
            "title": "Personality analysis based on multi-characteristic EEG signals",
            "why": "Represents core neuroaffective methodology—using high-fidelity neural signals to model personality-emotion interplay, advancing biometric foundations for personalized affective AI."
          },
          {
            "title": "SMIIP-NV: A Multi-Annotation Non-Verbal Expressive Speech Corpus in Mandarin for LLM-Based Speech Synthesis",
            "why": "Highlights emerging direction in affective speech: large-scale, multi-annotated, culturally grounded corpus development enabling LLM-integrated emotion-aware synthesis—bridging linguistics, prosody, and generative AI."
          },
          {
            "title": "An automated assessment framework for atypical prosody and stereotyped idiosyncratic phrases related to autism spectrum disorder",
            "why": "Seminal early work establishing clinical speech biomarkers for ASD; foundational for later papers on inclusive speech emotion recognition and prosodic affect modeling in neurodiverse populations."
          },
          {
            "title": "Joint Training on Multiple Datasets With Inconsistent Labeling Criteria for Facial Expression Recognition",
            "why": "Captures key methodological thrust: developing robust, annotation-agnostic emotion models—critical for real-world deployment where affect labeling is inherently subjective and dataset-specific."
          },
          {
            "title": "HSVRS: A Virtual Reality System of the Hide-and-Seek Game to Enhance Gaze Fixation Ability for Autistic Children",
            "why": "Demonstrates affective HCI in clinical VR—leveraging familiarity, attachment cues, and emotion-mediated attentional training to target core social-attention deficits in ASD."
          },
          {
            "title": "Assessing the Social Skills of Children with Autism Spectrum Disorder via Language-Image Pre-training Models",
            "why": "Signals strategic evolution toward foundation models—applying language-vision pretraining to infer emotion-related social skills from multimodal behavioral data, enabling scalable clinical assessment."
          },
          {
            "title": "EEG-based emotion recognition using a temporal-difference minimizing neural network",
            "why": "Highly cited technical contribution showing deep learning innovation for time-series affect modeling—optimizing temporal dynamics critical for real-time, physiologically grounded emotion inference."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 357,
        "interesting_works_count": 34,
        "new_works_count": 0,
        "deduped_works_count": 357,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5100351449.json"
    },
    {
      "identity": {
        "name": "Peipei Song",
        "orcid": "https://orcid.org/0000-0001-6764-3375",
        "google_scholar": "https://scholar.google.com/citations?user=aiIZ_MYAAAAJ",
        "openalex_author_id": "A5015177224",
        "openalex_author_url": "https://openalex.org/A5015177224"
      },
      "affiliation": {
        "last_known_institution": "Hefei University of Technology",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 68,
        "cited_by_count": 818,
        "h_index": 13,
        "i10_index": 16
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "affective multimodal understanding",
            "weight": 1
          },
          {
            "name": "emotion-aware natural language generation",
            "weight": 0.95
          },
          {
            "name": "video-based affective computing",
            "weight": 0.92
          },
          {
            "name": "affective multimodal generation",
            "weight": 0.88
          },
          {
            "name": "emotion cause inference",
            "weight": 0.85
          },
          {
            "name": "emotion representation learning",
            "weight": 0.82
          },
          {
            "name": "multimodal emotion modeling",
            "weight": 0.79
          },
          {
            "name": "BCI-driven affective neurofeedback for depression/anxiety post-stroke",
            "weight": 0.75
          }
        ],
        "trend_summary": "Peipei Song's research centers on affective AI with strong emphasis on multimodal (especially video-vision-language) emotion understanding and generation, progressively evolving from emotion recognition toward causal reasoning, subjective interpretation, and clinical translation—particularly in neurorehabilitation via BCI. A consistent thread is bridging objective signals (e.g., visual, EEG) with subjective emotional experience, while addressing challenges like emotion-label noise, cross-modal alignment, bias mitigation, and psychologically grounded evaluation. Recent work shows increasing integration of clinical neuroscience and real-time adaptive systems.",
        "representative_papers": [
          {
            "title": "Emotional Video Captioning With Vision-Based Emotion Interpretation Network",
            "why": "Highly cited foundational work establishing vision-language grounding with affective semantics for video captioning; anchors the core theme of multimodal affective generation."
          },
          {
            "title": "Multi-round Mutual Emotion-Cause Pair Extraction for Emotion-Attributed Video Captioning",
            "why": "Introduces iterative, causally grounded emotion modeling in video captioning—key step toward emotion reasoning beyond recognition."
          },
          {
            "title": "Subjective-Objective Emotion-Correlated Generation Network for Subjective Video Captioning",
            "why": "Represents the frontier of subjective-emotion modeling, explicitly aligning internal affective states with external perceptual signals."
          },
          {
            "title": "Integrative neurorehabilitation using brain-computer interface: From motor function to mental health after stroke",
            "why": "Marks a major domain expansion into clinical affective neurotechnology, demonstrating translational impact with highest citation count (5)."
          },
          {
            "title": "Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark",
            "why": "Defines next-generation evaluation standards—psychologically grounded, multi-turn, and predictive—shaping the field’s methodological rigor."
          },
          {
            "title": "Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion Reasoning",
            "why": "Addresses critical challenge of conflicting emotion signals across modalities, advancing robust cross-modal integration and bias mitigation."
          },
          {
            "title": "Emotion-Prior Awareness Network for Emotional Video Captioning",
            "why": "Early influential architecture embedding explicit emotion priors into generation, laying groundwork for emotion-aware NLG."
          },
          {
            "title": "Alleviating Confirmation Bias in Learning with Noisy Labels via Two-Network Collaboration",
            "why": "Highlights methodological innovation in affective ML—robust training under realistic label noise, enabling reliable emotion classification."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 61,
        "interesting_works_count": 11,
        "new_works_count": 0,
        "deduped_works_count": 61,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5015177224.json"
    },
    {
      "identity": {
        "name": "Qin Jin",
        "orcid": "https://orcid.org/0000-0001-6486-6020",
        "google_scholar": "https://scholar.google.com/citations?user=8UkYbCMAAAAJ",
        "openalex_author_id": "A5009985839",
        "openalex_author_url": "https://openalex.org/A5009985839"
      },
      "affiliation": {
        "last_known_institution": "Renmin University of China",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 238,
        "cited_by_count": 4961,
        "h_index": 32,
        "i10_index": 93
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Multimodal affective computing",
            "weight": 1
          },
          {
            "name": "Explainable and interpretable emotion AI",
            "weight": 0.97
          },
          {
            "name": "Emotion-aware natural language processing",
            "weight": 0.93
          },
          {
            "name": "Temporal and continuous emotion modeling",
            "weight": 0.89
          },
          {
            "name": "Human-centered affective AI for well-being",
            "weight": 0.85
          },
          {
            "name": "Robust and low-resource emotion recognition",
            "weight": 0.81
          },
          {
            "name": "Cross-modal and cross-cultural emotion analysis",
            "weight": 0.76
          },
          {
            "name": "Physiological and biosignal-based affect modeling",
            "weight": 0.72
          }
        ],
        "trend_summary": "Qin Jin's research centers on multimodal affective computing, with strong emphasis on interpretability, human-centered design, and real-world robustness. Over time, the work has evolved from early acoustic/visual/text fusion and dimensional emotion prediction toward sophisticated temporal modeling, reasoning chains (e.g., ECR-Chain), human-in-the-loop evaluation, and clinical or mental health applications (e.g., ESCoT). Recent directions integrate LLMs for affective reasoning, style- and sentiment-guided generation, and benchmarks emphasizing temporal dynamics and in-the-wild generalization. A consistent thread is bridging cognitive/psychological principles with deep learning—especially via graph networks, distillation, domain adaptation, and physiological grounding.",
        "representative_papers": [
          {
            "title": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation",
            "why": "Highly influential (255 citations); pioneered graph-based multimodal fusion for dialogue emotion, establishing a foundational architecture widely adopted in later work."
          },
          {
            "title": "Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities",
            "why": "Landmark contribution (159 citations) to robust multimodal affective AI, addressing real-world modality dropout—a persistent challenge in deployment."
          },
          {
            "title": "ESCoT: Towards Interpretable Emotional Support Dialogue Systems",
            "why": "Key pivot toward human-centered, clinically grounded affective AI (9 citations but high impact in mental health NLP; explicitly targets interpretability and emotional well-being)."
          },
          {
            "title": "ECR-Chain: Advancing Generative Language Models to Better Emotion-Cause Reasoners through Reasoning Chains",
            "why": "Represents the cutting edge of cognition-informed NLP—integrating causal reasoning, chain-of-thought, and psychology-driven explanation in emotion understanding."
          },
          {
            "title": "Valence and Arousal Estimation based on Multimodal Temporal-Aware Features for Videos in the Wild",
            "why": "Seminal work (24 citations) on continuous, in-the-wild emotion estimation using temporal-aware multimodal fusion—benchmark-setting for real-world affect dynamics."
          },
          {
            "title": "Memobert: Pre-Training Model with Prompt-Based Learning for Multimodal Emotion Recognition",
            "why": "Bridges pretraining paradigms and low-resource affective modeling (33 citations); introduces prompt-based adaptation for multimodal emotion—key for generalization."
          },
          {
            "title": "Adaptive Temporal Motion Guided Graph Convolution Network for Micro-expression Recognition",
            "why": "Exemplifies fine-grained, nonverbal affect analysis with neurocognitive grounding (5 citations, 2024); integrates motion dynamics, graph learning, and micro-expression specificity."
          },
          {
            "title": "Exploring Interpretability in Deep Learning for Affective Computing: A Comprehensive Review",
            "why": "Definitive 2025 survey (5 citations so far) synthesizing explainability across emotion AI—signals maturation of the field and Jin’s leadership in interpretability frameworks."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 208,
        "interesting_works_count": 46,
        "new_works_count": 0,
        "deduped_works_count": 208,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5009985839.json"
    },
    {
      "identity": {
        "name": "Qunxi Dong",
        "orcid": "https://orcid.org/0000-0002-0484-3019",
        "google_scholar": "https://scholar.google.com/citations?user=ocu6_PMAAAAJ",
        "openalex_author_id": "A5057680942",
        "openalex_author_url": "https://openalex.org/A5057680942"
      },
      "affiliation": {
        "last_known_institution": "Beijing Institute of Technology",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 64,
        "cited_by_count": 1027,
        "h_index": 19,
        "i10_index": 34
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective neuroscience-informed machine learning",
            "weight": 1
          },
          {
            "name": "Computational psychiatry",
            "weight": 0.97
          },
          {
            "name": "AI-driven affective state classification for mental health",
            "weight": 0.94
          },
          {
            "name": "Multimodal emotion sensing and fusion (EEG/physiological/fMRI)",
            "weight": 0.91
          },
          {
            "name": "Causal and disentangled representation learning for affect",
            "weight": 0.88
          },
          {
            "name": "Translational affective neuroscience and neuromodulation (e.g., taVNS)",
            "weight": 0.85
          },
          {
            "name": "Affective brain network modeling and biomarker discovery",
            "weight": 0.82
          },
          {
            "name": "Explainable and robust neuro-AI for clinical emotion assessment",
            "weight": 0.79
          }
        ],
        "trend_summary": "Qunxi Dong's research centers on bridging affective neuroscience, computational psychiatry, and AI—particularly through interpretable, causally grounded, and clinically translatable models of emotion and affective disorders. A strong longitudinal thread is the use of EEG and multimodal physiological signals (fMRI, structural MRI, autonomic markers) to develop robust, cross-subject, distributionally invariant classifiers for depression, schizophrenia, and emotional dysregulation. Recent work increasingly emphasizes causal disentanglement, dynamic functional connectivity, neuromodulation (taVNS), and brain-inspired AI (e.g., SNNs), reflecting a maturing focus on mechanism-aware, deployable tools for real-world mental healthcare.",
        "representative_papers": [
          {
            "title": "WDANet: Wasserstein Distribution Inspired Dynamic Adversarial Network for EEG-Based Cross-Domain Depression Recognition",
            "why": "Exemplifies core integration of distributional robustness, cross-domain generalization, and affective neuroscience priors in AI for depression recognition."
          },
          {
            "title": "Resting-state dynamic functional connectivity in major depressive disorder: A systematic review",
            "why": "Synthesizes foundational neurodynamic principles underlying affective dysfunction, anchoring empirical work in computational psychiatry theory."
          },
          {
            "title": "Advancements in Affective Disorder Detection: Using Multimodal Physiological Signals and Neuromorphic Computing Based on SNNs",
            "why": "Represents convergence of multimodal sensing, brain-inspired AI, and real-time clinical assessment—highlighting translational innovation."
          },
          {
            "title": "Constraint-Driven Causal Representation Learning for Vigilance Robust Estimation in Brain–Computer Interface",
            "why": "Embodies the latest methodological thrust: integrating causal constraints with disentangled latent representations for robust affective BCI."
          },
          {
            "title": "Exploring the Alleviating Effects of taVNS on Negative Emotions: An EEG Study",
            "why": "Demonstrates rigorous translational affective neuroscience—linking non-invasive neuromodulation, neurophysiological markers, and emotion regulation."
          },
          {
            "title": "The Three-Lead EEG Sensor: Introducing an EEG-Assisted Depression Diagnosis System Based on Ant Lion Optimization",
            "why": "Highlights pragmatic clinical translation—low-cost wearable EEG systems enhanced by bio-inspired optimization for accessible diagnosis."
          },
          {
            "title": "Artificial intelligence in psychiatry research, diagnosis, and therapy",
            "why": "Seminal high-impact review establishing the conceptual and clinical framework for AI-augmented affective psychiatry."
          },
          {
            "title": "Fundamentals of Computational Psychophysiology: Theory and Methodology",
            "why": "Foundational methodological treatise defining the interdisciplinary paradigm—integrating psychophysiology, emotion theory, and computational inference."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 60,
        "interesting_works_count": 26,
        "new_works_count": 0,
        "deduped_works_count": 60,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5057680942.json"
    },
    {
      "identity": {
        "name": "Shengping Zhang",
        "orcid": "https://orcid.org/0000-0001-5200-3420",
        "google_scholar": "https://scholar.google.com/citations?user=hMNsT8sAAAAJ",
        "openalex_author_id": "A5084025984",
        "openalex_author_url": "https://openalex.org/A5084025984"
      },
      "affiliation": {
        "last_known_institution": "Harbin Institute of Technology - Weihai",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 273,
        "cited_by_count": 8669,
        "h_index": 46,
        "i10_index": 99
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective human-computer interaction",
            "weight": 1
          },
          {
            "name": "Emotion-aware AI systems",
            "weight": 0.95
          },
          {
            "name": "Multimodal emotion recognition",
            "weight": 0.9
          },
          {
            "name": "Real-time affective animation",
            "weight": 0.85
          },
          {
            "name": "Clinical applications of emotion AI (e.g., mental health support)",
            "weight": 0.8
          },
          {
            "name": "Cognitive-affective modeling in robotics",
            "weight": 0.75
          },
          {
            "name": "Computational affective science",
            "weight": 0.7
          },
          {
            "name": "Affective influences on financial decision-making",
            "weight": 0.65
          }
        ],
        "trend_summary": "Shengping Zhang's research spans over two decades, evolving from foundational work in affective influences on behavior (e.g., finance, online personality) to cutting-edge, multimodal, real-time emotion-aware AI systems. A clear longitudinal trend emerges: integration of emotion modeling with perception (vision, audio, behavioral signals), generation (digital humans, animation), and application domains—especially clinical mental health support and robotics. Recent work emphasizes dynamic, adaptive, and real-time affective capabilities across modalities, reflecting a shift toward deployable, ethically grounded affective intelligence.",
        "representative_papers": [
          {
            "title": "REA-Listener: Real-Time Listening Head Generation with Dynamic Emotion Modeling and Flexible Modality Adaptation",
            "why": "Exemplifies the convergence of real-time affective animation, multimodal adaptation, and dynamic emotion modeling—the most current and technically advanced direction in Zhang's portfolio."
          },
          {
            "title": "Research advancements on emotionally and intellectually integrated digital humans and robotics",
            "why": "Synthesizes cross-domain vision for affective-cognitive integration in embodied AI, highlighting clinical and robotic applications central to recent agenda."
          },
          {
            "title": "Introduction to the Special Issue on Multimodal Machine Learning for Human Behavior Analysis",
            "why": "Foundational survey framing multimodal emotion recognition and behavioral signal processing as core methodological pillars for human-centered AI."
          },
          {
            "title": "Behavior Recognition in Mouse Videos using Contextual Features Encoded by Spatial-temporal Stacked Fisher Vectors",
            "why": "Pioneering computational affective science work bridging animal models, vision-based behavioral biomarkers, and emotion assessment—demonstrating early methodological innovation in affective measurement."
          },
          {
            "title": "On the Undergraduates' Network Personality",
            "why": "Early exploration of affective computing in social media contexts, foreshadowing later interest in digital well-being and emotion-related personality in interactive environments."
          },
          {
            "title": "Behavioral Finance and Psychology",
            "why": "Earliest identified paper, establishing foundational interest in affective drivers of complex human decision-making—linking psychology, economics, and emotion regulation."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 242,
        "interesting_works_count": 6,
        "new_works_count": 0,
        "deduped_works_count": 242,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5084025984.json"
    },
    {
      "identity": {
        "name": "Shiguang Shan",
        "orcid": "https://orcid.org/0000-0002-8348-392X",
        "google_scholar": "https://scholar.google.com/citations?user=Vkzd7MIAAAAJ",
        "openalex_author_id": "A5050297728",
        "openalex_author_url": "https://openalex.org/A5050297728"
      },
      "affiliation": {
        "last_known_institution": "University of Chinese Academy of Sciences",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 726,
        "cited_by_count": 34405,
        "h_index": 93,
        "i10_index": 369
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "affective computing",
            "weight": 0.127
          },
          {
            "name": "Affective computing",
            "weight": 0.037
          },
          {
            "name": "facial expression analysis",
            "weight": 0.022
          },
          {
            "name": "facial behavior analysis",
            "weight": 0.022
          },
          {
            "name": "multimodal emotion analysis",
            "weight": 0.015
          },
          {
            "name": "emotion-aware computer vision",
            "weight": 0.011
          },
          {
            "name": "facial emotion analysis",
            "weight": 0.011
          },
          {
            "name": "multimodal emotion recognition",
            "weight": 0.011
          }
        ],
        "trend_summary": "AI summary unavailable. Generated by frequency fallback.",
        "representative_papers": [
          {
            "title": "The CAS-PEAL Large-Scale Chinese Face Database and Baseline Evaluations",
            "why": "Highly cited (1006)."
          },
          {
            "title": "Occlusion Aware Facial Expression Recognition Using CNN With Attention Mechanism",
            "why": "Highly cited (872)."
          },
          {
            "title": "Learning Expressionlets on Spatio-temporal Manifold for Dynamic Facial Expression Recognition",
            "why": "Highly cited (393)."
          },
          {
            "title": "RhythmNet: End-to-End Heart Rate Estimation From Face via Spatial-Temporal Representation",
            "why": "Highly cited (379)."
          },
          {
            "title": "Deeply Learning Deformable Facial Action Parts Model for Dynamic Expression Analysis",
            "why": "Highly cited (339)."
          },
          {
            "title": "Facial Expression Recognition with Inconsistently Annotated Datasets",
            "why": "Highly cited (330)."
          },
          {
            "title": "AU-inspired Deep Networks for Facial Expression Feature Learning",
            "why": "Highly cited (232)."
          },
          {
            "title": "AU-aware Deep Networks for facial expression recognition",
            "why": "Highly cited (226)."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 615,
        "interesting_works_count": 68,
        "new_works_count": 0,
        "deduped_works_count": 615,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5050297728.json"
    },
    {
      "identity": {
        "name": "Sicheng Zhao",
        "orcid": "https://orcid.org/0000-0001-5843-6411",
        "google_scholar": "https://scholar.google.com/citations?user=LJiQRJIAAAAJ",
        "openalex_author_id": "A5051149140",
        "openalex_author_url": "https://openalex.org/A5051149140"
      },
      "affiliation": {
        "last_known_institution": "Harbin Institute of Technology",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 250,
        "cited_by_count": 8419,
        "h_index": 46,
        "i10_index": 113
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "multimodal emotion analysis",
            "weight": 1
          },
          {
            "name": "affective computing",
            "weight": 0.98
          },
          {
            "name": "cross-domain emotion understanding",
            "weight": 0.95
          },
          {
            "name": "emotion representation disentanglement",
            "weight": 0.92
          },
          {
            "name": "label-efficient and low-resource affective learning",
            "weight": 0.89
          },
          {
            "name": "personalized emotion modeling",
            "weight": 0.86
          },
          {
            "name": "affective multimodal AI",
            "weight": 0.83
          },
          {
            "name": "causal and dynamic emotion modeling in dialogue",
            "weight": 0.8
          }
        ],
        "trend_summary": "Sicheng Zhao's research centers on advancing robust, human-centered affective AI—particularly through multimodal (vision, language, audio) emotion understanding grounded in psychological principles. A strong longitudinal thread is cross-domain adaptation and generalization (e.g., source-free, open-vocabulary, unsupervised), increasingly guided by disentanglement, information bottleneck, and causal reasoning. Recent work emphasizes efficiency (label-, data-, and demonstration-efficient learning), personalization (via personality, physiology, or user context), and responsible deployment (privacy, uncertainty, subjectivity, ethics). There is a clear evolution from early image-centric emotion prediction toward foundation-model-aware, dialogue- and healthcare-integrated, and psychologically informed affective systems.",
        "representative_papers": [
          {
            "title": "Multimodal Sentiment Analysis With Image-Text Interaction Network",
            "why": "Highly cited foundational work (194 citations) establishing core architecture patterns for cross-modal affective grounding—exemplifies his long-standing leadership in multimodal emotion analysis."
          },
          {
            "title": "Unlocking the Emotional World of Visual Media: An Overview of the Science, Research, and Impact of Understanding Emotion",
            "why": "Seminal high-impact survey (67 citations) synthesizing psychology, computer vision, and ethics—defines his integrative, multidisciplinary framing of emotion AI."
          },
          {
            "title": "Dynamic Causal Disentanglement Model for Dialogue Emotion Detection",
            "why": "Represents cutting-edge convergence of causality, dynamics, and disentanglement in conversational affect—highly cited (12) and conceptually rich."
          },
          {
            "title": "EmotionGAN",
            "why": "Early influential generative approach (75 citations) pioneering unsupervised emotion transfer—marks origin of his domain adaptation and generative affective modeling trajectory."
          },
          {
            "title": "Predicting Continuous Probability Distribution of Image Emotions in Valence-Arousal Space",
            "why": "Foundational probabilistic, dimensional emotion modeling (40 citations) bridging psychology and ML—established his focus on subjectivity and continuous affect representation."
          },
          {
            "title": "Personalized Emotion Recognition by Personality-Aware High-Order Learning of Physiological Signals",
            "why": "Key paper (78 citations) anchoring personalized, trait-informed affective modeling—demonstrates his consistent emphasis on individual variability."
          },
          {
            "title": "An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability",
            "why": "Signals timely pivot toward foundation models and human-in-the-loop affective reasoning—reflects current focus on efficient, controllable, bias-aware LLM-based emotion understanding."
          },
          {
            "title": "Affective Image Content Analysis: Two Decades Review and New Perspectives",
            "why": "Landmark review (157 citations) tracing evolution of visual affective computing—underscores his role as a field-shaping synthesizer and forward-looking theorist."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 225,
        "interesting_works_count": 60,
        "new_works_count": 0,
        "deduped_works_count": 225,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5051149140.json"
    },
    {
      "identity": {
        "name": "Su-Jing Wang",
        "orcid": "https://orcid.org/0000-0002-8774-6328",
        "google_scholar": "https://scholar.google.com/citations?user=BW-_LKsAAAAJ",
        "openalex_author_id": "A5066403927",
        "openalex_author_url": "https://openalex.org/A5066403927"
      },
      "affiliation": {
        "last_known_institution": "Institute of Psychology Chinese Academy of Sciences",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 233,
        "cited_by_count": 8449,
        "h_index": 45,
        "i10_index": 116
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "micro-expression detection and analysis",
            "weight": 1
          },
          {
            "name": "affective computing",
            "weight": 0.98
          },
          {
            "name": "nonverbal emotion detection",
            "weight": 0.95
          },
          {
            "name": "affective signal processing",
            "weight": 0.92
          },
          {
            "name": "emotion-aware AI",
            "weight": 0.89
          },
          {
            "name": "multimodal emotion understanding",
            "weight": 0.86
          },
          {
            "name": "physiological measurement of emotion",
            "weight": 0.83
          },
          {
            "name": "social media sentiment analysis",
            "weight": 0.79
          }
        ],
        "trend_summary": "Su-Jing Wang's research centers on the computational modeling of *spontaneous, subtle, and concealed affective states*, with micro-expression analysis as the dominant thread across two decades. Her work bridges psychology and AI—emphasizing physiological grounding (especially facial EMG), real-world robustness (in-the-wild spotting, long videos, ecological databases like CAS(ME)³), and multimodal integration (vision-language, vision-physiology). Recent evolution includes expanding into LLM-based affective NLP for mental health and public health monitoring, while maintaining core focus on nonverbal behavioral signals, cross-modal validation, and ethically grounded, psychologically informed AI.",
        "representative_papers": [
          {
            "title": "CAS(ME)<sup>3</sup>: A Third Generation Facial Spontaneous Micro-Expression Database with Depth Information and High Ecological Validity",
            "why": "Landmark database work establishing high-fidelity, ecologically valid benchmarks—epitomizes her commitment to rigorous, psychology-grounded data infrastructure for micro-expression research."
          },
          {
            "title": "A Main Directional Mean Optical Flow Feature for Spontaneous Micro-Expression Recognition",
            "why": "Highly cited foundational method (504 citations) introducing directional optical flow for capturing subtle motion dynamics—core technical contribution shaping subsequent spatiotemporal modeling."
          },
          {
            "title": "MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering",
            "why": "Represents latest frontier: integrating micro-expression analysis with vision-language models, signaling strategic expansion into multimodal affective reasoning and explainable affective AI."
          },
          {
            "title": "Could Micro-Expressions Be Quantified? Electromyography Gives Affirmative Evidence",
            "why": "Key cross-modal validation paper linking facial EMG to micro-expression intensity—exemplifies her integrative approach bridging physiology and computer vision for objective quantification."
          },
          {
            "title": "Social Media Analysis for Mental Health using Large Language Models",
            "why": "Highlights timely pivot toward affective NLP and public health applications, demonstrating adaptability of core affective AI principles to textual, population-scale emotion sensing."
          },
          {
            "title": "Dual-ATME: Dual-Branch Attention Network for Micro-Expression Recognition",
            "why": "Highly influential deep learning architecture (46 citations) advancing attention mechanisms for fine-grained temporal-spatial cue extraction—benchmark model in modern MER."
          },
          {
            "title": "3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video Sequences using Temporal Oriented Reference Frame",
            "why": "Seminal work (50 citations) tackling real-world challenges—long videos, joint macro/micro spotting—pioneering temporal modeling for spontaneous affect detection."
          },
          {
            "title": "CASME II: An Improved Spontaneous Micro-Expression Database and the Baseline Evaluation",
            "why": "Foundational resource (930 citations) that catalyzed the field; reflects her enduring impact on standardization, reproducibility, and clinical/security applicability of affective computing."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 223,
        "interesting_works_count": 63,
        "new_works_count": 0,
        "deduped_works_count": 223,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5066403927.json"
    },
    {
      "identity": {
        "name": "Tong Zhang",
        "orcid": "https://orcid.org/0000-0002-7025-6365",
        "google_scholar": "https://scholar.google.com/citations?user=-neWbpUAAAAJ",
        "openalex_author_id": "A5100378800",
        "openalex_author_url": "https://openalex.org/A5100378800"
      },
      "affiliation": {
        "last_known_institution": "South China University of Technology",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 369,
        "cited_by_count": 12484,
        "h_index": 49,
        "i10_index": 137
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "EEG-based emotion recognition",
            "weight": 1
          },
          {
            "name": "Interpretable and explainable AI for affective computing",
            "weight": 0.97
          },
          {
            "name": "Multimodal emotion analysis (EEG, speech, text, face, gait, eye movement)",
            "weight": 0.93
          },
          {
            "name": "Graph neural networks for affective signal modeling",
            "weight": 0.91
          },
          {
            "name": "Uncertainty-aware and robust emotion modeling",
            "weight": 0.88
          },
          {
            "name": "Clinical affective AI and mental health applications (depression, Alzheimer’s, addiction)",
            "weight": 0.85
          },
          {
            "name": "Cross-subject and domain-adaptive affective modeling",
            "weight": 0.82
          },
          {
            "name": "Affective natural language processing and dialogue systems",
            "weight": 0.79
          }
        ],
        "trend_summary": "Tong Zhang's research centers on rigorous, neurophysiologically grounded affective AI—especially EEG-driven emotion decoding—with strong emphasis on interpretability, multimodal fusion, and clinical translation. A dominant trend is the integration of graph neural networks with physiological signal modeling to capture brain functional connectivity, while consistently addressing real-world challenges: individual variability (via cross-subject/domain adaptation), annotation noise and subjectivity (via uncertainty modeling and crowdsourcing-aware architectures), and clinical validity (e.g., depression detection, neurodegenerative disorder biomarkers). Recent work increasingly bridges computational psychiatry and human-centered AI—leveraging LLMs, dynamic graphs, disentanglement, and orthogonal representation learning—not just for accuracy but for mechanistic insight and deployable trustworthiness.",
        "representative_papers": [
          {
            "title": "GDDN: Graph Domain Disentanglement Network for Generalizable EEG Emotion Recognition",
            "why": "Highly cited (53), exemplifies core synthesis of graph learning, domain generalization, and EEG-based affective modeling; addresses fundamental cross-subject transfer challenge."
          },
          {
            "title": "Grop: Graph Orthogonal Purification Network for EEG Emotion Recognition",
            "why": "Highest citation count (26) among recent works, introduces orthogonal representation learning to isolate emotion-specific neural features—key for interpretability and robustness."
          },
          {
            "title": "CiABL: Completeness-Induced Adaptative Broad Learning for Cross-Subject Emotion Recognition With EEG and Eye Movement Signals",
            "why": "Top-cited paper (46) in multimodal cross-subject setting; pioneers adaptive broad learning for physiological signal fusion with interpretability focus."
          },
          {
            "title": "Hierarchical Dynamic Graph Convolutional Network With Interpretability for EEG-Based Emotion Recognition",
            "why": "Foundational high-impact work (86 citations); established interpretable dynamic graph modeling of time-varying brain connectivity for emotion."
          },
          {
            "title": "Ugan: Uncertainty-Guided Graph Augmentation Network for EEG Emotion Recognition",
            "why": "Seminal uncertainty-aware framework (12 citations, high influence); integrates aleatoric uncertainty estimation directly into graph augmentation for robust decoding."
          },
          {
            "title": "ACM-GNN: Adaptive Cluster-Oriented Modularity Graph Neural Network for EEG Depression Detection",
            "why": "Direct clinical translation—uses GNNs to extract neuro-affective biomarkers for major depressive disorder, bridging computational psychiatry and graph AI."
          },
          {
            "title": "Emotion Recognition in Conversation Based on a Dynamic Complementary Graph Convolutional Network",
            "why": "Represents shift toward contextual, dialogue-level affect modeling using dynamic graphs; integrates commonsense and multimodal context for conversational emotion."
          },
          {
            "title": "TFAGL: A Novel Agent Graph Learning Method Using Time-Frequency EEG for Major Depressive Disorder Detection",
            "why": "Innovates time-frequency graph learning for clinical EEG; demonstrates how spectral-temporal graph structures improve diagnostic specificity in depression."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 345,
        "interesting_works_count": 58,
        "new_works_count": 0,
        "deduped_works_count": 345,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5100378800.json"
    },
    {
      "identity": {
        "name": "Weidong Chen",
        "orcid": null,
        "google_scholar": "https://scholar.google.com/citations?user=Z-vKGdoAAAAJ",
        "openalex_author_id": "A5100357376",
        "openalex_author_url": "https://openalex.org/A5100357376"
      },
      "affiliation": {
        "last_known_institution": "Ministry of Education",
        "last_known_country": "Taiwan",
        "source": "openalex"
      },
      "metrics": {
        "works_count": 650,
        "cited_by_count": 7578,
        "h_index": 45,
        "i10_index": 166
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "explainable affective vision",
            "weight": 1
          },
          {
            "name": "affective computing",
            "weight": 0.95
          },
          {
            "name": "multimodal emotion modeling",
            "weight": 0.9
          },
          {
            "name": "emotion-aware AI",
            "weight": 0.85
          },
          {
            "name": "affective brain-computer interfaces",
            "weight": 0.8
          },
          {
            "name": "Natural language processing for emotion",
            "weight": 0.75
          },
          {
            "name": "compact pretrained models for affective tasks",
            "weight": 0.7
          },
          {
            "name": "emotion-informed information retrieval",
            "weight": 0.65
          }
        ],
        "trend_summary": "Weidong Chen's research demonstrates a sustained, evolving focus on affective computing—spanning neurophysiological, visual, auditory, and linguistic modalities—with increasing emphasis on interpretability, multimodality, and efficiency. Early work (2015) pioneered emotion-informed BCI for rapid visual search; recent work (2023–2025) shifts toward foundation-model-driven, explainable, and compact affective AI—including MLLM-based emotion representation (EmoVerse), compact speech emotion models (Vesper), and contrastive NLP methods for affective style transfer. A consistent thread is bridging low-level affective signals (EEG, speech, text, image) with high-level semantic and cognitive interpretations.",
        "representative_papers": [
          {
            "title": "EmoVerse: A MLLMs-Driven Emotion Representation Dataset for Interpretable Visual Emotion Analysis",
            "why": "Exemplifies the current frontier: integrating multimodal large language models with knowledge graphs to enable interpretable, continuous/discrete emotion representation in vision—a synthesis of explainable affective vision, multimodal modeling, and structured emotion annotation."
          },
          {
            "title": "Vesper: A Compact and Effective Pretrained Model for Speech Emotion Recognition",
            "why": "Highlights the shift toward efficient, self-supervised affective AI—addressing deployment constraints while maintaining performance, representing core themes of emotion-aware AI and compact pretrained modeling."
          },
          {
            "title": "An Iterative Approach for EEG-Based Rapid Face Search: A Refined Retrieval by Brain Computer Interfaces",
            "why": "Foundational work demonstrating early innovation in affective BCI and neuroadaptive systems, establishing the long-standing interest in decoding cognitive-affective signals for real-time, emotion-informed interaction."
          },
          {
            "title": "Text Style Transfer with Contrastive Transfer Pattern Mining",
            "why": "Bridges affective NLP and representation learning, showing how stylistic affective content can be disentangled and transferred—key for controllable, sentiment-aware language generation."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 624,
        "interesting_works_count": 5,
        "new_works_count": 0,
        "deduped_works_count": 624,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5100357376.json"
    },
    {
      "identity": {
        "name": "Wenming Zheng",
        "orcid": "https://orcid.org/0000-0002-7764-5179",
        "google_scholar": "https://scholar.google.com/citations?user=k5fqIogAAAAJ",
        "openalex_author_id": "A5029771864",
        "openalex_author_url": "https://openalex.org/A5029771864"
      },
      "affiliation": {
        "last_known_institution": "Southeast University",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 311,
        "cited_by_count": 10764,
        "h_index": 52,
        "i10_index": 149
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Cross-domain and cross-corpus emotion recognition",
            "weight": 1
          },
          {
            "name": "EEG-based affective computing and brain-computer interfaces",
            "weight": 0.98
          },
          {
            "name": "Micro-expression recognition in the wild",
            "weight": 0.95
          },
          {
            "name": "Multimodal emotion analysis (audio-visual-physiological)",
            "weight": 0.93
          },
          {
            "name": "Robust and generalizable affective feature learning",
            "weight": 0.91
          },
          {
            "name": "Clinical affective computing for mental health assessment",
            "weight": 0.89
          },
          {
            "name": "Affective speech processing (SER, emotional voice conversion)",
            "weight": 0.87
          },
          {
            "name": "Graph-based modeling of affective signals (GNNs for EEG/facial/AU)",
            "weight": 0.85
          }
        ],
        "trend_summary": "Wenming Zheng's research centers on robust, generalizable, and clinically relevant affective computing—spanning signal processing, deep learning, and neuroscience. A dominant theme is overcoming domain shift across datasets, speakers, modalities, and populations (e.g., infants, elderly, ASD), achieved via advanced domain adaptation, transfer learning, and invariant representation learning. There is strong emphasis on physiological signals—especially EEG—with pioneering use of graph neural networks to model brain connectivity dynamics. Micro-expression recognition emerges as a signature niche, focusing on subtle, spontaneous, and real-world cues using magnification-aware, sparse, and uncertainty-aware architectures. Multimodality (audio-visual-physiological-fNIRS) and clinical translation (depression, autism, elderly mental health) are consistently integrated. Recent work increasingly incorporates privacy-preserving AI (federated learning), controllable generation (emotion intensity, voice conversion), and group-level/context-aware emotion modeling.",
        "representative_papers": [
          {
            "title": "EEG Emotion Recognition Using Dynamical Graph Convolutional Neural Networks",
            "why": "Highly cited foundational paper (1377 citations) introducing dynamic GNNs for EEG, establishing Zheng's leadership in neuroaffective graph modeling."
          },
          {
            "title": "GMSS: Graph-Based Multi-Task Self-Supervised Learning for EEG Emotion Recognition",
            "why": "Landmark self-supervised approach (170 citations) enabling generalizable, label-efficient EEG emotion decoding—exemplifies shift toward scalable, data-efficient neuroaffective AI."
          },
          {
            "title": "A Novel Bi-Hemispheric Discrepancy Model for EEG Emotion Recognition",
            "why": "Seminal neuroscience-inspired architecture (326 citations) modeling hemispheric asymmetry, bridging computational models with affective neuroscience principles."
          },
          {
            "title": "Progressive graph convolution network for EEG emotion recognition",
            "why": "Highly influential (72 citations) progressive GNN framework demonstrating architectural innovation in spatiotemporal brain signal decoding."
          },
          {
            "title": "NaME: A Natural Micro-expression Dataset for Micro-expression Recognition in the Wild",
            "why": "Represents strategic move toward ecological validity—introducing a large-scale, naturalistic micro-expression benchmark addressing long-standing 'lab vs. wild' gaps."
          },
          {
            "title": "Towards Federated Learning Driving Technology for Privacy-Preserving Micro-Expression Recognition",
            "why": "Highlights growing focus on ethical AI—applying federated learning to sensitive behavioral biometrics, balancing performance with privacy."
          },
          {
            "title": "Multi-Level Segment Fusion Based on Adaptive Time-Window Selection for Multimodal Personality-Aware Elderly Depression Detection",
            "why": "Exemplifies clinical translation: integrating multimodal sensing, adaptive temporal modeling, and gerontechnology for real-world mental health screening."
          },
          {
            "title": "Speech Swin-Transformer: Exploring a Hierarchical Transformer with Shifted Windows for Speech Emotion Recognition",
            "why": "Demonstrates methodological evolution—adopting vision-inspired transformers for speech, achieving high impact (30 citations) in SER architecture design."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 295,
        "interesting_works_count": 190,
        "new_works_count": 0,
        "deduped_works_count": 295,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5029771864.json"
    },
    {
      "identity": {
        "name": "Xiao Sun",
        "orcid": "https://orcid.org/0000-0001-9750-7032",
        "google_scholar": null,
        "openalex_author_id": "A5088062069",
        "openalex_author_url": "https://openalex.org/A5088062069"
      },
      "affiliation": {
        "last_known_institution": "Hefei University of Technology",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 399,
        "cited_by_count": 7255,
        "h_index": 38,
        "i10_index": 92
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective computing",
            "weight": 1
          },
          {
            "name": "Emotion recognition",
            "weight": 0.97
          },
          {
            "name": "Affective dialogue systems",
            "weight": 0.93
          },
          {
            "name": "Multimodal emotion analysis",
            "weight": 0.91
          },
          {
            "name": "Emotion-aware AI",
            "weight": 0.88
          },
          {
            "name": "Clinical emotion AI",
            "weight": 0.85
          },
          {
            "name": "Affective NLP",
            "weight": 0.82
          },
          {
            "name": "Computational empathy",
            "weight": 0.79
          }
        ],
        "trend_summary": "Xiao Sun's research centers on affective computing as a unifying paradigm, with strong emphasis on real-world, clinically grounded, and multimodal emotion modeling. Key trends include: (1) shifting from static facial expression recognition toward dynamic, longitudinal, and context-aware emotion modeling in dialogue and behavioral streams; (2) increasing integration of clinical psychology—especially depression, anxiety, and mental health screening—via behavioral biomarkers (gait, gaze, facial dynamics, stock behavior) and clinical NLP; (3) methodological innovation in graph-based, transformer-based, and reinforcement learning–enhanced affective models; and (4) growing focus on human-centered, empathetic, and ethically aware AI systems for mental health support, emotional conversation, and real-time affective interaction.",
        "representative_papers": [
          {
            "title": "WELD: A Large-Scale Longitudinal Dataset of Emotional Dynamics for Ubiquitous Affective Computing",
            "why": "Introduces the first large-scale longitudinal emotional dynamics dataset, reflecting Xiao Sun’s focus on real-world, time-resolved affective modeling beyond snapshot recognition."
          },
          {
            "title": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation",
            "why": "Exemplifies the integration of large language models with clinical psychology—highlighting the direction of emotion-aware LLMs and computational psychiatry."
          },
          {
            "title": "Facial Depression Estimation via Multi-Cue Contrastive Learning",
            "why": "Bridges clinical emotion AI and multimodal signal processing, using contrastive learning to extract depression-relevant facial biomarkers—core to Xiao Sun’s translational affective computing agenda."
          },
          {
            "title": "Detecting Depression With Heterogeneous Graph Neural Network in Clinical Interview Transcript",
            "why": "Demonstrates high-impact application of graph neural networks to clinical NLP for automated depression detection—showcasing synergy between affective NLP and mental healthcare."
          },
          {
            "title": "Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation",
            "why": "Represents the convergence of computational empathy, RL-based affective behavior modeling, and human-centered dialogue design."
          },
          {
            "title": "AST-GCN: Augmented Spatial Temporal Graph Convolutional Neural Network for Gait Emotion Recognition",
            "why": "Illustrates innovative use of nonverbal behavioral signals (gait) with spatiotemporal deep learning—expanding emotion recognition beyond face/speech/text."
          },
          {
            "title": "UniEmoX: Cross-Modal Semantic-Guided Large-Scale Pretraining for Universal Scene Emotion Perception",
            "why": "Signals a strategic move toward foundation models for universal affective perception, integrating psychological priors and cross-modal pretraining."
          },
          {
            "title": "BSGM: Psychological State Analysis Method Based on Deep Learning and Stock Market Dynamic Data Augmentation",
            "why": "Highlights an unconventional yet rigorous extension of affective computing into behavioral/financial data as proxy for latent psychological states—underscoring interdisciplinary innovation."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 381,
        "interesting_works_count": 104,
        "new_works_count": 0,
        "deduped_works_count": 381,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5088062069.json"
    },
    {
      "identity": {
        "name": "Xiaobai Li",
        "orcid": "https://orcid.org/0000-0003-4519-7823",
        "google_scholar": "https://scholar.google.com/citations?user=JTFfexYAAAAJ",
        "openalex_author_id": "A5101691493",
        "openalex_author_url": "https://openalex.org/A5101691493"
      },
      "affiliation": {
        "last_known_institution": "Zhejiang University",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 147,
        "cited_by_count": 8260,
        "h_index": 41,
        "i10_index": 71
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "multimodal emotion analysis",
            "weight": 1
          },
          {
            "name": "micro-expression analysis",
            "weight": 0.98
          },
          {
            "name": "affective computing",
            "weight": 0.96
          },
          {
            "name": "explainable AI for emotion analysis",
            "weight": 0.92
          },
          {
            "name": "clinical affective computing",
            "weight": 0.89
          },
          {
            "name": "remote physiological sensing for emotion inference",
            "weight": 0.87
          },
          {
            "name": "cross-cultural affective computing",
            "weight": 0.84
          },
          {
            "name": "fine-grained emotion recognition from body cues",
            "weight": 0.81
          }
        ],
        "trend_summary": "Xiaobai Li's research centers on multimodal, clinically grounded, and ethically aware affective AI—progressing from foundational micro-expression analysis toward interpretable, cross-culturally robust, and contactless emotion inference. Key trends include: (1) strong emphasis on integrating facial, bodily, audio, textual, and physiological signals (e.g., rPPG, EEG); (2) growing focus on clinical translation—especially depression, stress, and engagement detection in real-world settings (e.g., online meetings, mental health monitoring); (3) methodological innovation in disentanglement, fusion architectures (e.g., VISTANet, TCCT-Net), reinforcement learning (AffectGPT-R1), and vision-language models; (4) increasing attention to cultural adaptivity, privacy preservation, and psychological validity (e.g., SMG, iMiGUE, Privacy-Phys); and (5) sustained leadership in benchmarking via Grand Challenges (MEGC/MAC series) and high-impact dataset curation (CASME II, 4DME, SMG).",
        "representative_papers": [
          {
            "title": "CASME II: An Improved Spontaneous Micro-Expression Database and the Baseline Evaluation",
            "why": "Foundational dataset paper with highest citation count (930), establishing gold-standard spontaneous micro-expression resources and catalyzing the field."
          },
          {
            "title": "Video-Based Remote Physiological Measurement via Cross-Verified Feature Disentangling",
            "why": "Highly cited (197) methodological breakthrough enabling reliable contactless emotion inference from video via rPPG, underpinning many later clinical and remote-affect works."
          },
          {
            "title": "SMG: A Micro-gesture Dataset Towards Spontaneous Body Gestures for Emotional Stress State Analysis",
            "why": "Seminal body-based affect resource (48 citations), expanding beyond face/voice to psychologically-grounded micro-gestures for hidden stress detection."
          },
          {
            "title": "MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering",
            "why": "Flagship 2025 challenge integrating micro-expression analysis with vision-language reasoning—showcasing next-gen multimodal, interpretable, and task-aware affective AI."
          },
          {
            "title": "VISTANet: VIsual Spoken Textual Additive Net for Interpretable Multimodal Emotion Recognition",
            "why": "Exemplifies core direction of explainable multimodal fusion, unifying visual, spoken, and textual signals with transparency for real-world deployment."
          },
          {
            "title": "Multimodal Interpretable Depression Analysis Using Visual, Physiological, Audio and Textual Data",
            "why": "Represents clinical translation focus—highest-cited clinical paper (3 citations in early 2025, but reflects mature methodology applied to depression biomarkers)."
          },
          {
            "title": "Cross-Cultural Nuances of Micro-Expressions and Action Units: A Comparative Study",
            "why": "Highlights emerging priority on cultural robustness—addressing critical gaps in anatomical vs. affective interpretation across populations."
          },
          {
            "title": "Editorial: Towards Emotion AI to next generation healthcare and education",
            "why": "Synthesizes overarching vision—bridging technical advances with trust, ethics, adoption barriers, and human-centered applications in high-stakes domains."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 127,
        "interesting_works_count": 54,
        "new_works_count": 0,
        "deduped_works_count": 127,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5101691493.json"
    },
    {
      "identity": {
        "name": "Xiaolan Fu",
        "orcid": "https://orcid.org/0000-0002-6944-1037",
        "google_scholar": "https://scholar.google.com/citations?user=tM8ln5IAAAAJ",
        "openalex_author_id": "A5015329251",
        "openalex_author_url": "https://openalex.org/A5015329251"
      },
      "affiliation": {
        "last_known_institution": "Shanghai Jiao Tong University",
        "last_known_country": "China",
        "source": "openalex"
      },
      "metrics": {
        "works_count": 339,
        "cited_by_count": 7989,
        "h_index": 41,
        "i10_index": 101
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective Computing and Micro-expression Analysis",
            "weight": 0.95
          },
          {
            "name": "Affective Neuroscience and Neural Correlates",
            "weight": 0.9
          },
          {
            "name": "Emotion-Cognition Interaction",
            "weight": 0.85
          },
          {
            "name": "Facial Expression Recognition and Perception",
            "weight": 0.8
          },
          {
            "name": "Social Emotion and Moral Psychology",
            "weight": 0.7
          },
          {
            "name": "Clinical Affective Science and Psychopathology",
            "weight": 0.65
          },
          {
            "name": "Affective AI and Multimodal Modeling",
            "weight": 0.6
          },
          {
            "name": "Cross-cultural Emotion Research",
            "weight": 0.5
          }
        ],
        "trend_summary": "Research commenced (2002-2006) with foundational work on media effects, group decision-making, and basic emotion measurement scales. During 2007-2011, focus expanded to cross-cultural trust, implicit emotional learning, and neural mechanisms of insight. The 2012-2016 window marked a pivotal shift towards affective computing, notably micro-expression recognition via the CASME database, alongside neural correlates of emotion perception using ERP. In 2017-2021, deep learning techniques enhanced micro-expression spotting, while research broadened to include moral cognition, sense of agency, and masked expression analysis during the pandemic. The latest period (2022-2025) demonstrates integration of affective AI with neuroscience, emphasizing multimodal emotion recognition, clinical psychopathology, and neural decoding of social agency. Current work prioritizes evaluating emotion grounding in foundation models and refining physiological measurement of covert affect.",
        "representative_papers": [
          {
            "title": "CASME II: An Improved Spontaneous Micro-Expression Database and the Baseline Evaluation",
            "why": "Establishes the foundational CASME database series critical for spontaneous micro-expression research."
          },
          {
            "title": "A Main Directional Mean Optical Flow Feature for Spontaneous Micro-Expression Recognition",
            "why": "Represents key algorithmic innovation in optical flow features for affective computing."
          },
          {
            "title": "The interaction between cognition and emotion",
            "why": "Early theoretical paper defining the core emotion-cognition interaction framework."
          },
          {
            "title": "MESNet: A Convolutional Neural Network for Spotting Multi-Scale Micro-Expression Intervals in Long Videos",
            "why": "Demonstrates the evolution towards deep learning architectures for video-based emotion spotting."
          },
          {
            "title": "CAS(ME)3: A Third Generation Facial Spontaneous Micro-Expression Database with Depth Information and High Ecological Validity",
            "why": "Shows continuity and advancement in dataset construction with multimodal depth information."
          },
          {
            "title": "Decoding the temporal representation of facial expression in face-selective regions",
            "why": "Highlights advanced neural decoding techniques using MEG for time-resolved emotion processing."
          },
          {
            "title": "EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models",
            "why": "Reflects the latest shift towards evaluating emotion grounding in foundation models and AI."
          },
          {
            "title": "The role of arousal in the estimation of time‐to‐collision of threatening stimuli",
            "why": "Exemplifies research on affective modulation of perceptual timing and survival-related cognition."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 318,
        "interesting_works_count": 149,
        "new_works_count": 2,
        "deduped_works_count": 318,
        "ai_called_count": 2,
        "summary_recomputed": true
      },
      "profile_path": "data/researchers/profiles/A5015329251.json"
    },
    {
      "identity": {
        "name": "Xun Chen",
        "orcid": "https://orcid.org/0000-0002-4922-8116",
        "google_scholar": "https://scholar.google.com/citations?user=aBnUWyQAAAAJ",
        "openalex_author_id": "A5100451602",
        "openalex_author_url": "https://openalex.org/A5100451602"
      },
      "affiliation": {
        "last_known_institution": "University of Science and Technology of China",
        "last_known_country": "China",
        "source": "openalex"
      },
      "metrics": {
        "works_count": 687,
        "cited_by_count": 19312,
        "h_index": 63,
        "i10_index": 284
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective neuroscience",
            "weight": 1
          },
          {
            "name": "Biomedical signal processing",
            "weight": 0.95
          },
          {
            "name": "Deep learning for affective signals",
            "weight": 0.92
          },
          {
            "name": "Affective brain-computer interfaces",
            "weight": 0.88
          },
          {
            "name": "Emotion recognition from neural signals",
            "weight": 0.85
          },
          {
            "name": "Robust and generalizable emotion decoding from EEG",
            "weight": 0.82
          },
          {
            "name": "Interpretable and efficient AI for affective computing",
            "weight": 0.78
          },
          {
            "name": "Multimodal affective signal analysis",
            "weight": 0.75
          }
        ],
        "trend_summary": "Xun Chen's research centers on advancing affective neuroscience through computational methods, with a strong emphasis on EEG-based emotion recognition. His work bridges biomedical signal processing and modern deep learning—especially capsule networks, transformers, neural architecture search, and attention mechanisms—to build robust, interpretable, and deployable affective BCI systems. A clear trend is the evolution from classical ML (e.g., SVM) toward data-efficient, cross-subject, open-world, and neurocognitively grounded deep models. Recent focus includes handling noisy labels, subject independence, edge deployment, self-supervised learning, and integration of physiological modalities (EMG, PPG, GVS) for richer affective inference.",
        "representative_papers": [
          {
            "title": "EEG-Based Emotion Recognition via Channel-Wise Attention and Self Attention",
            "why": "Highly cited foundational paper introducing attention mechanisms to EEG-based emotion recognition; established interpretability and channel-specific modeling as core themes."
          },
          {
            "title": "EEG-Based Emotion Recognition via Transformer Neural Architecture Search",
            "why": "Most cited paper (115 citations); exemplifies convergence of automated ML, transformer architectures, and affective signal decoding—highlighting scalability and optimization rigor."
          },
          {
            "title": "TC-Net: A Transformer Capsule Network for EEG-based emotion recognition",
            "why": "Seminal fusion of transformer and capsule networks for EEG; reflects innovation in structural modeling of spatio-temporal emotional dynamics."
          },
          {
            "title": "Toward Open-World Electroencephalogram Decoding Via Deep Learning: A comprehensive survey",
            "why": "Authoritative survey framing key challenges (cross-subject, cross-session, limited labels, real-world deployment); defines the field’s current frontiers."
          },
          {
            "title": "Emotion recognition from EEG based on multi-task learning with capsule network and attention mechanism",
            "why": "Highly cited (124) integration of multi-task learning, capsules, and attention—demonstrating principled architectural design for neurophysiological emotion modeling."
          },
          {
            "title": "PulseGAN: Learning to Generate Realistic Pulse Waveforms in Remote Photoplethysmography",
            "why": "Most cited paper overall (222 citations); expands affective computing beyond EEG into remote, contactless cardiovascular biomarkers—showcasing multimodal affect sensing."
          },
          {
            "title": "Efficient Ocular Artifacts Removal From EEG Recordings Using State-Space Model",
            "why": "Latest work (2025) highlighting enduring focus on signal fidelity and preprocessing rigor—critical for reliable downstream affective decoding."
          },
          {
            "title": "Multi-channel EEG-based emotion recognition in the presence of noisy labels",
            "why": "Addresses practical real-world challenge of label uncertainty in affective datasets—underscoring shift toward robust, clinically viable models."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 659,
        "interesting_works_count": 21,
        "new_works_count": 0,
        "deduped_works_count": 659,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5100451602.json"
    },
    {
      "identity": {
        "name": "Yanyan Zhao",
        "orcid": "https://orcid.org/0000-0001-5852-7962",
        "google_scholar": "https://scholar.google.com/citations?user=mEdfAYoAAAAJ",
        "openalex_author_id": "A5031911989",
        "openalex_author_url": "https://openalex.org/A5031911989"
      },
      "affiliation": {
        "last_known_institution": "Yunnan Normal University",
        "last_known_country": "China",
        "source": "openalex"
      },
      "metrics": {
        "works_count": 119,
        "cited_by_count": 2708,
        "h_index": 22,
        "i10_index": 33
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "Affective NLP for mental health applications",
            "weight": 1
          },
          {
            "name": "Explainable and cognition-aware emotion modeling",
            "weight": 0.97
          },
          {
            "name": "Multimodal and uncertainty-aware affective analysis",
            "weight": 0.93
          },
          {
            "name": "Fine-grained, aspect- and utterance-level sentiment/emotion analysis",
            "weight": 0.89
          },
          {
            "name": "Causal, contextual, and iterative emotion modeling in dialogue",
            "weight": 0.85
          },
          {
            "name": "Label-efficient and weakly supervised affective learning",
            "weight": 0.81
          },
          {
            "name": "Cross-domain and domain-adaptive emotion/sentiment modeling",
            "weight": 0.76
          }
        ],
        "trend_summary": "Yanyan Zhao's research centers on human-centered affective AI, with a strong clinical and cognitive grounding—especially in mental health (e.g., depression, cognitive distortions) using social media and conversational data. Her work consistently bridges NLP, multimodal learning, and clinical psychology: advancing explainability via cognitive theories (e.g., cognitive distortion), integrating psychiatric scales into LLMs, modeling emotion distributions and causes in context, and addressing data scarcity through weak/low-shot supervision. A clear evolution is visible—from foundational aspect-based sentiment analysis (2010s) to clinically aligned, uncertainty-aware, multimodal, and causally grounded affective systems (2022–2024).",
        "representative_papers": [
          {
            "title": "Scale-CoT: Integrating LLM with Psychiatric Scales for Analyzing Mental Health Issues on Social Media",
            "why": "Exemplifies her signature cross-disciplinary integration—bridging clinical psychiatry (standardized scales) and LLMs for explainable, domain-grounded mental health analysis on social media."
          },
          {
            "title": "CauAIN: Causal Aware Interaction Network for Emotion Recognition in Conversations",
            "why": "Highly cited (65) and foundational for causal reasoning in affective dialogue systems—demonstrates her focus on context-aware, mechanistic emotion modeling beyond surface classification."
          },
          {
            "title": "Cognitive distortion based explainable depression detection and analysis technologies for the adolescent internet users on social media",
            "why": "Highlights her clinical translation focus—using linguistically grounded cognitive distortion markers to build interpretable, adolescent-specific depression detection tools."
          },
          {
            "title": "Toward Label-Efficient Emotion and Sentiment Analysis",
            "why": "Most cited paper (34) in her corpus; anchors her methodological emphasis on practical deployment constraints—weak supervision, annotation ambiguity, and domain adaptation."
          },
          {
            "title": "Data Uncertainty-Aware Learning for Multimodal Aspect-based Sentiment Analysis",
            "why": "Represents her cutting-edge direction: unifying multimodality, fine-grained aspect modeling, and explicit uncertainty quantification for robust affective systems."
          },
          {
            "title": "A Topic-Enhanced Approach for Emotion Distribution Forecasting in Conversations",
            "why": "Illustrates her shift toward predictive, distributional, and discourse-aware emotion modeling—moving beyond single-label classification to dynamic emotional trajectories."
          },
          {
            "title": "C2D2 Dataset: A Resource for the Cognitive Distortion Analysis and Its Impact on Mental Health",
            "why": "Signals her commitment to resource creation grounded in clinical theory—enabling reproducible, cognition-emotion interaction research in NLP."
          },
          {
            "title": "An Iterative Emotion Interaction Network for Emotion Recognition in Conversations",
            "why": "Early high-impact work (46 citations) establishing her long-standing focus on dialogue context, iterative inference, and inter-utterance emotion dynamics."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 115,
        "interesting_works_count": 40,
        "new_works_count": 0,
        "deduped_works_count": 115,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5031911989.json"
    },
    {
      "identity": {
        "name": "Yue Gao",
        "orcid": "https://orcid.org/0000-0002-4971-590X",
        "google_scholar": "https://scholar.google.com/citations?user=UTDfWocAAAAJ",
        "openalex_author_id": "A5100602494",
        "openalex_author_url": "https://openalex.org/A5100602494"
      },
      "affiliation": {
        "last_known_institution": "Tsinghua University",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 583,
        "cited_by_count": 19017,
        "h_index": 70,
        "i10_index": 261
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "affective computing",
            "weight": 1
          },
          {
            "name": "personalized emotion modeling",
            "weight": 0.92
          },
          {
            "name": "emotion-aware AI",
            "weight": 0.88
          },
          {
            "name": "multimodal emotion analysis",
            "weight": 0.85
          },
          {
            "name": "social media emotion modeling",
            "weight": 0.81
          },
          {
            "name": "affective computer vision",
            "weight": 0.77
          },
          {
            "name": "clinical affective disorders",
            "weight": 0.73
          },
          {
            "name": "affective neuroscience",
            "weight": 0.69
          }
        ],
        "trend_summary": "Yue Gao's research bridges foundational affective science with cutting-edge AI, evolving from early work on image/song emotion recognition (2009–2016) toward increasingly sophisticated, multimodal, and context-aware systems — especially personalized, robust, and open-world affect recognition. A strong longitudinal thread integrates psychophysiological signals, graph-based learning (hypergraphs), and cross-modal fusion. Recent work expands into clinical translation (anxiety, depression, insomnia), aging populations, organizational affect dynamics, and non-human affect (e.g., livestock welfare), reflecting a convergent trajectory: human-centered, ethically grounded, and computationally rigorous affective intelligence.",
        "representative_papers": [
          {
            "title": "Personalized Emotion Recognition by Personality-Aware High-Order Learning of Physiological Signals",
            "why": "Highest-cited paper (78 citations); establishes core paradigm of personality-aware, physiology-based personalization — foundational for later work in robust and adaptive affective AI."
          },
          {
            "title": "Continuous Probability Distribution Prediction of Image Emotions via Multitask Shared Sparse Regression",
            "why": "Most highly cited (191 citations); pioneered continuous, distributional, and subjective emotion modeling in vision — defining contribution to affective computer vision."
          },
          {
            "title": "Predicting Personalized Image Emotion Perceptions in Social Networks",
            "why": "Highly influential (168 citations); early integration of social context + personalization in affective computing, foreshadowing current human-centered AI focus."
          },
          {
            "title": "The differential orbitofrontal activity and connectivity between atypical and typical major depressive disorder",
            "why": "Key clinical neuroscience contribution (4 citations but high conceptual weight); bridges computational affective models with neurobiological validation in mood disorders."
          },
          {
            "title": "Recognition of aggressive behavior of group-housed pigs based on CNN-GRU hybrid model with spatio-temporal attention mechanism",
            "why": "Most cited paper (50 citations); exemplifies domain expansion into non-human affect and real-world AI deployment for welfare — highlights methodological rigor in wild-condition affect recognition."
          },
          {
            "title": "A whirlpool of emotion: How entrepreneurs’ empathy affects employees‘ emotional exhaustion",
            "why": "Represents cross-disciplinary reach into organizational affect science; demonstrates integration of interpersonal emotion transmission theory with empirical modeling."
          },
          {
            "title": "Exploring the mechanism of action of huoermai essential oil for plateau insomnia based on the camp/CREB/BDNF/gabaergic pathway",
            "why": "Signals translational ethnopharmacology direction — unique integration of traditional medicine, neurobiological pathways, and emotion-related sleep disorders."
          },
          {
            "title": "Variance-Aware Bi-Attention Expression Transformer for Open-Set Facial Expression Recognition in the Wild",
            "why": "Latest methodological advance (2023); embodies current focus on robustness, generalizability, and transformer-based architectures for unconstrained affect recognition."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 539,
        "interesting_works_count": 32,
        "new_works_count": 0,
        "deduped_works_count": 539,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5100602494.json"
    },
    {
      "identity": {
        "name": "Zheng Lian",
        "orcid": "https://orcid.org/0000-0001-9477-0599",
        "google_scholar": "https://scholar.google.com/citations?user=S34nWz0AAAAJ",
        "openalex_author_id": "A5001973434",
        "openalex_author_url": "https://openalex.org/A5001973434"
      },
      "affiliation": {
        "last_known_institution": "Tongji University",
        "last_known_country": "China",
        "source": "orcid"
      },
      "metrics": {
        "works_count": 152,
        "cited_by_count": 2485,
        "h_index": 27,
        "i10_index": 49
      },
      "topic_summary": {
        "top_research_directions": [
          {
            "name": "affective multimodal AI",
            "weight": 1
          },
          {
            "name": "robust multimodal AI",
            "weight": 0.97
          },
          {
            "name": "LLM-driven affective computing",
            "weight": 0.94
          },
          {
            "name": "explainable and interpretable emotion modeling",
            "weight": 0.91
          },
          {
            "name": "multimodal emotion recognition",
            "weight": 0.88
          },
          {
            "name": "emotion-personality and emotion-intent joint modeling",
            "weight": 0.85
          },
          {
            "name": "responsible and ethical affective AI",
            "weight": 0.82
          },
          {
            "name": "self-supervised representation learning for affect",
            "weight": 0.79
          }
        ],
        "trend_summary": "Zheng Lian's research centers on advancing affective computing through multimodal, robust, and human-centered AI—especially at the intersection of large language models (LLMs) and emotion understanding. Key trends include: (1) integrating vision-language models (e.g., GPT-4V, AffectGPT, Emotion-LLaMA) for zero-shot, open-vocabulary, and instruction-tuned emotion reasoning; (2) addressing real-world challenges like missing modalities, noise, and data scarcity via dynamic curriculum learning, contrastive masking, graph completion, and modality-invariant fusion; (3) prioritizing interpretability, explainability, and psychological grounding in model design and evaluation; (4) expanding beyond discrete classification to fine-grained, dimensional, compound, and joint emotion-intent-personality modeling; and (5) emphasizing responsible deployment—including ethics, privacy, clinical applicability (e.g., depression severity prediction), and cross-cultural/human-centered design.",
        "representative_papers": [
          {
            "title": "GPT-4V with emotion: A zero-shot benchmark for Generalized Emotion Recognition",
            "why": "Seminal work benchmarking foundation vision-language models on emotion recognition, establishing zero-shot generalization as a new paradigm in affective AI."
          },
          {
            "title": "Contrastive Learning Based Modality-Invariant Feature Acquisition for Robust Multimodal Emotion Recognition With Missing Modalities",
            "why": "Highly cited (59) methodological contribution to robustness—introduces principled contrastive learning for handling incomplete multimodal inputs, a recurring challenge across Lian’s work."
          },
          {
            "title": "HiCMAE: Hierarchical Contrastive Masked Autoencoder for self-supervised Audio-Visual Emotion Recognition",
            "why": "Landmark self-supervised architecture (42 citations) enabling label-efficient, cross-modal emotion representation learning—exemplifies the shift from supervised to self-supervised affective foundations."
          },
          {
            "title": "Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning",
            "why": "Pioneering integration of LLM instruction tuning into multimodal emotion tasks, bridging generative AI capabilities with affective reasoning and explainability."
          },
          {
            "title": "GCNet: Graph Completion Network for Incomplete Multimodal Learning in Conversation",
            "why": "Highly influential (119 citations) graph-based solution for missing modality robustness in dialogue, reflecting Lian’s focus on structured, psychologically informed emotional dynamics."
          },
          {
            "title": "AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models",
            "why": "Foundational resource paper introducing a dedicated MLLM benchmark and dataset—catalyzing standardized evaluation and reproducible progress in affective MLLMs."
          },
          {
            "title": "MRAC 2025: 3rd International Workshop on Multimodal, Generative and Responsible Affective Computing",
            "why": "Flagship workshop co-organized by Lian, explicitly unifying three pillars: multimodality, generative modeling, and responsibility—encapsulating his holistic research vision."
          },
          {
            "title": "Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities",
            "why": "Represents the latest evolution of robustness research—introducing adaptive, difficulty-aware training to improve generalization under heterogeneous data quality and modality loss."
          }
        ]
      },
      "stats": {
        "analyzed_works_count": 136,
        "interesting_works_count": 76,
        "new_works_count": 0,
        "deduped_works_count": 136,
        "ai_called_count": 0,
        "summary_recomputed": false
      },
      "profile_path": "data/researchers/profiles/A5001973434.json"
    }
  ]
}
