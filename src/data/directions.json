[
  {
    "name": "Affect Recognition",
    "description": "Automatic recognition of human emotional states from facial expressions, body language, vocal cues, and physiological signals. This direction covers both discrete emotion categories (happiness, sadness, anger, etc.) and continuous dimensional models (valence-arousal space).",
    "related_teams": ["MIT Media Lab", "Carnegie Mellon University", "National University of Singapore", "University of Pittsburgh"],
    "recommended_papers": [
      "A Survey on Affective Computing and Intelligent Interaction",
      "Spontaneous Facial Action Unit Detection in the Wild"
    ]
  },
  {
    "name": "Multimodal Emotion Analysis",
    "description": "Jointly modeling multiple modalities — text, audio, video, and physiological signals — for robust emotion understanding. Research explores fusion strategies, modality alignment, and cross-modal learning to go beyond what any single channel can provide.",
    "related_teams": ["University of Cambridge", "Carnegie Mellon University", "MIT Media Lab"],
    "recommended_papers": [
      "Cross-cultural Emotion Understanding in Multimodal Signals",
      "Multimodal Sentiment Analysis with Transformer Fusion",
      "Dyadic Conversation Emotion Dynamics Modeling"
    ]
  },
  {
    "name": "Speech Emotion Recognition",
    "description": "Extracting emotional information from spoken language using acoustic features (pitch, energy, MFCCs) and recently large pre-trained speech models. Applications span clinical mental health monitoring, call center analytics, and voice interfaces.",
    "related_teams": ["University of Toronto"],
    "recommended_papers": [
      "Affective Speech Foundation Model for Clinical Scenarios"
    ]
  },
  {
    "name": "Micro-expression",
    "description": "Detection and recognition of brief, involuntary facial expressions that reveal concealed emotions. Due to their subtle and transient nature (typically 1/25 to 1/5 second), micro-expressions require specialized high-frame-rate capture and temporal analysis methods.",
    "related_teams": ["National University of Singapore", "University of Pittsburgh"],
    "recommended_papers": [
      "Facial Micro-expression Benchmark for Real-world Affective States"
    ]
  },
  {
    "name": "Emotion-aware LLM",
    "description": "Integrating emotional understanding and generation into large language models for empathetic dialogue, emotion cause analysis, sentiment reasoning, and emotionally-grounded text generation. A rapidly growing area as LLMs become central to conversational AI.",
    "related_teams": ["Tsinghua University", "Peking University"],
    "recommended_papers": [
      "Large Language Models for Emotion-centric Human-Computer Interaction",
      "Emotion Cause Analysis in Conversational Settings"
    ]
  },
  {
    "name": "Affective HCI",
    "description": "Designing and evaluating interactive systems that sense, interpret, and respond to user affective states. Research covers adaptive interfaces, social robots, empathetic virtual agents, and affective feedback loops in educational and therapeutic contexts.",
    "related_teams": ["Tsinghua University", "Stanford University"],
    "recommended_papers": [
      "Large Language Models for Emotion-centric Human-Computer Interaction",
      "Social Emotional Intelligence for Human-Robot Interaction"
    ]
  },
  {
    "name": "Physiological Signal Sensing",
    "description": "Using body-worn or remote sensors to capture physiological signals — including EEG, ECG, GSR, and fNIRS — as continuous and often involuntary indicators of affective state. Research addresses signal processing, noise robustness, and person-independent modeling.",
    "related_teams": ["MIT Media Lab", "Zhejiang University"],
    "recommended_papers": [
      "Physiological Affect Sensing under Naturalistic Conditions",
      "EEG-based Emotion Recognition with Graph Neural Networks"
    ]
  }
]
